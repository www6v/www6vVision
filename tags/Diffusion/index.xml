<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Diffusion on Vision</title>
    <link>https://www6v.github.io/www6vVision/tags/Diffusion/</link>
    <description>Recent content in Diffusion on Vision</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 16 Apr 2024 16:26:40 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vVision/tags/Diffusion/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>pix2pix-zero</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Pix2pix/</link>
      <pubDate>Tue, 16 Apr 2024 16:26:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Pix2pix/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2302.03027&#34;&gt;Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/pix2pixzero/pix2pix-zero&#34;&gt;pix2pix-zero&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&lt;br&gt;&#xA;&lt;a href=&#34;https://pix2pixzero.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;官网有个介绍视频 看过不错&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  Method[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/e9wsarfq.bmp&#34; alt=&#34;e9wsarfq.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;上图展示了pix2pix-zero方法的概述，这是一个将图片从猫变成狗的图像到图像的翻译例子。首先，使用规范化的DDIM反转来得到一个反转的噪声映射，这是由BLIP图像字幕（caption）网络和CLIP文本嵌入模型自动生成的文本嵌入引导的。然后，使用原始文本嵌入去噪以获得交叉注意力图，作为输入图像结构的参考（顶部行）。接下来，使用编辑后的文本嵌入去噪，通过损失函数确保这些交叉注意力图与参考交叉注意力图相匹配（第二行）。这确保了编辑图像的结构与原始图像相比不会发生剧烈变化。没有交叉注意力引导的去噪示例显示在第三行，导致结构上的大偏差。此可视化强调了在编辑过程中保持图像原始结构的交叉注意力的重要性。&lt;/p&gt;&#xA;&lt;h1 id=&#34;discovering-edit-directions-2&#34;&gt;&#xA;  Discovering edit directions [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#discovering-edit-directions-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/hwe1rl5e.bmp&#34; alt=&#34;hwe1rl5e.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/684673737&#34;&gt;pix2pix-zero：零样本图像到图像转换&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/x1131230123/article/details/132169755&#34;&gt;【深度学习】【风格迁移】Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;实战&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/docs/diffusers/v0.13.0/en/api/pipelines/stable_diffusion/pix2pix_zero&#34;&gt;Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>InstructPix2Pix</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Instruct/</link>
      <pubDate>Tue, 16 Apr 2024 16:25:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Instruct/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2211.09800&#34;&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;Repo&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;method&#34;&gt;&#xA;  &lt;strong&gt;Method&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;这章就讲两件事：1）如何&lt;strong&gt;生成数据集&lt;/strong&gt;（章节3.1）；2）如何&lt;strong&gt;基于上一步生成的训练数据，训练一个图像编辑扩散模型&lt;/strong&gt;（章节3.2）；&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/94xbrqlt.bmp&#34; alt=&#34;94xbrqlt.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/72gzhwfl.bmp&#34; alt=&#34;72gzhwfl.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/655135961&#34;&gt;InstructPix2Pix：用指令给图像做修改&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.notion.so/5d04c1bbd0d34be4b3fb0087cb670efd?pvs=21&#34;&gt;InstructPix2Pix&lt;/a&gt;  diffusers&#xA;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1Go4y1M7cK?p=3&#34;&gt;InstructPix2Pix&lt;/a&gt; V&#xA;&lt;a href=&#34;https://github.com/www6v/Diffusion_Training_Examples&#34;&gt;Code Repo&lt;/a&gt; git&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prompt-to-Prompt</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Prompt/</link>
      <pubDate>Tue, 16 Apr 2024 16:25:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Prompt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;&lt;a href=&#34;https://prompt-to-prompt.github.io/ptp_files/Prompt-to-Prompt_preprint.pdf&#34;&gt;PROMPT-TO-PROMPT IMAGE EDITING&#xA;WITH CROSS-ATTENTION CONTROL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/google/prompt-to-prompt/&#34;&gt;Prompt-to-Prompt&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://prompt-to-prompt.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;示意1&#34;&gt;&#xA;  示意[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%a4%ba%e6%84%8f1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/gvn5hr6b.bmp&#34; alt=&#34;gvn5hr6b.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;应用1&#34;&gt;&#xA;  应用[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;**应用方面：**Prompt-to-Prompt 这个方法是原理上的创新，应用方面只适用于“已经生成了一张大致满意的图，我们想对它进行部分修改”。&lt;strong&gt;但是对于“手头有一张来历不明的图，我们想对它进行修改”这个任务就很麻烦了&lt;/strong&gt;，因为很难去倒推这张图对应的prompt是啥。&lt;/p&gt;&#xA;&lt;p&gt;所以后续有一项工作叫 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/655135961&#34;&gt;InstructPix2Pix&lt;/a&gt;，作用是“&lt;strong&gt;一张来历不明的图，只要说‘把猫改成狗’，模型就能把画面里的猫改成狗，其他不变&lt;/strong&gt;。”非常好用，听说已经集成在 Stable Diffusion WebUI 里可以直接用了。&lt;/p&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  Method[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/vfk2k2bg.bmp&#34; alt=&#34;vfk2k2bg.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;上半部分，原版cross-attention，下半部分，本文的cross-attention&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/ck0g81ot.bmp&#34; alt=&#34;ck0g81ot.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/655372592&#34;&gt;Prompt-to-prompt：让生成的图像保持一致&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_40779727/article/details/136854062&#34;&gt;diffusion model(十四)： prompt-to-prompt 深度剖析&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work|实战)Image Editing</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEditWork/</link>
      <pubDate>Thu, 27 Jul 2023 09:29:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEditWork/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;总结&#34;&gt;&#xA;  总结&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prompt-to-Prompt&lt;br&gt;&#xA;train-free，察觉到了attention map的妙用&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;pix2pix-zero&lt;br&gt;&#xA;察觉到了attention map的妙用&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;InstructPix2Pix&lt;br&gt;&#xA;trainable，training数据基于Prompt-to-Prompt&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MGIE&lt;br&gt;&#xA;基于LMM&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>(综述)Image Editing</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEdit/</link>
      <pubDate>Thu, 06 Jul 2023 19:10:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEdit/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/abs/2402.17525&#34;&gt;《Diffusion Model-Based Image Editing: A Survey》&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods&#34;&gt;Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;图像编辑1&#34;&gt;&#xA;  图像编辑[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9b%be%e5%83%8f%e7%bc%96%e8%be%911&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;大类&#34;&gt;&#xA;  大类&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e7%b1%bb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从图片编辑的任务方面可以被分为3个大类&#xA;&lt;ul&gt;&#xA;&lt;li&gt;语义编辑semantic editing&lt;/li&gt;&#xA;&lt;li&gt;风格编辑stylistic editing&lt;/li&gt;&#xA;&lt;li&gt;结构编辑structural editing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;approaches&#34;&gt;&#xA;  APPROACHES&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#approaches&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TRAINING-BASED APPROACHES&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;InstructPix2Pix&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TESTING-TIME FINETUNING APPROACHES&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TRAINING AND FINETUNING FREE APPROACHES&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;论文2&#34;&gt;&#xA;  论文[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%872&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 复旦、南洋理工&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/xinchengshuai/Awesome-Image-Editing&#34;&gt;A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
