<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multimodal on Vision</title>
    <link>https://www6v.github.io/www6vVision/tags/multimodal/</link>
    <description>Recent content in Multimodal on Vision</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 04 Apr 2024 09:38:25 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vVision/tags/multimodal/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>多模态 系列</title>
      <link>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSeries/</link>
      <pubDate>Thu, 04 Apr 2024 09:38:25 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSeries/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;stage1-模块独立2&#34;&gt;&#xA;  Stage1: 模块独立[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage1-%e6%a8%a1%e5%9d%97%e7%8b%ac%e7%ab%8b2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/stage1.webp&#34; alt=&#34;stage1.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;model&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CLIP&lt;/li&gt;&#xA;&lt;li&gt;ViLT&lt;/li&gt;&#xA;&lt;li&gt;ALBEF&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;stage2-模块共享2&#34;&gt;&#xA;  Stage2: 模块共享[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage2-%e6%a8%a1%e5%9d%97%e5%85%b1%e4%ba%ab2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;model-1&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VLMO&lt;/li&gt;&#xA;&lt;li&gt;BLIP&lt;/li&gt;&#xA;&lt;li&gt;BLIP2&lt;/li&gt;&#xA;&lt;li&gt;BEiTv3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;stage3-范式统一2&#34;&gt;&#xA;  Stage3: 范式统一[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage3-%e8%8c%83%e5%bc%8f%e7%bb%9f%e4%b8%802&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;model-2&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Unified-IO&lt;/li&gt;&#xA;&lt;li&gt;Uni-Perceiver&lt;/li&gt;&#xA;&lt;li&gt;PaLi&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;总结-1&#34;&gt;&#xA;  总结 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/multimodal.webp&#34; alt=&#34;multimodal.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;&#xA;  Overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/653902791&#34;&gt;多模态大模型 CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP 系列解读&lt;/a&gt; ***&lt;/p&gt;</description>
    </item>
    <item>
      <title>DINO</title>
      <link>https://www6v.github.io/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/DINO/</link>
      <pubDate>Sun, 24 Mar 2024 11:18:19 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/DINO/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;dino&#34;&gt;&#xA;  DINO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dino&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dino&#34;&gt;https://github.com/facebookresearch/dino&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/EGCAX51FTyZrO7-e4y9Egg&#34;&gt;重塑自监督学习: DINO 网络如何颠覆视觉特征表示的常规方法&lt;/a&gt;   有动图&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;dinov2&#34;&gt;&#xA;  &lt;strong&gt;DINOv2&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dinov2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2304.07193&#34;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;https://github.com/facebookresearch/dinov2&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&#xA;&lt;a href=&#34;https://dinov2.metademolab.com/&#34;&gt;Project page&lt;/a&gt;&#xA;&lt;a href=&#34;https://pierrefdz.github.io/publications/dinov2/&#34;&gt;Project page&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考-1&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2314807&#34;&gt;DINOv2：无需微调，填补 SAM 的空白，支持多个下游任务&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/CVHub/article/details/130304078&#34;&gt;全网最详细的 DINOv2 论文解读来啦！&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43694096/article/details/135761460&#34;&gt;深度学习算法应用实战 | DINOv2 图像相似度实战&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/629042197&#34;&gt;视觉大模型DINOv2:自我监督学习的新领域&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/650234933&#34;&gt;DINOv2：无需微调，填补SAM空白&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mmpretrain.readthedocs.io/zh-cn/dev/papers/dinov2.html&#34;&gt;DINOv2 on mmpretrain&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey)多模态</title>
      <link>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSurvey/</link>
      <pubDate>Thu, 16 Mar 2023 12:45:30 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSurvey/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》 .Sep 2023   - microsoft&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings&#34;&gt;Computer Vision in the Wild (CVinW)&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;overview-0&#34;&gt;&#xA;  overview [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/overview.jpeg&#34; alt=&#34;overview.jpeg&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;视觉理解-1&#34;&gt;&#xA;  视觉理解 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%86%e8%a7%89%e7%90%86%e8%a7%a3-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/understanding.png&#34; alt=&#34;understanding.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/understanding-method.png&#34; alt=&#34;understanding-method.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;视觉生成-1&#34;&gt;&#xA;  视觉生成 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%86%e8%a7%89%e7%94%9f%e6%88%90-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;human-alignments-in-visual-generation--10&#34;&gt;&#xA;  Human Alignments in Visual Generation  [10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#human-alignments-in-visual-generation--10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;四种alignment的方式&lt;/p&gt;&#xA;&lt;h3 id=&#34;spatial-controllable-t2i-generation&#34;&gt;&#xA;  spatial controllable T2I generation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#spatial-controllable-t2i-generation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;结合位置分布的文字描述&lt;/strong&gt;（比较麻烦的用户交互，不仅需要文字，而且需要位置），常用于&lt;strong&gt;对位置要求比较高的创意设计（海报等）&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>BLIP-2</title>
      <link>https://www6v.github.io/www6vVision/docs/MLLM/VL/Blip/</link>
      <pubDate>Wed, 15 Mar 2023 23:00:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/MLLM/VL/Blip/</guid>
      <description>&lt;h1 id=&#34;blip-2&#34;&gt;&#xA;  BLIP-2&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#blip-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;overview-1&#34;&gt;&#xA;  Overview [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;用一个Qformer来提取图像特征（等同与Flamingo的perceiver resampler），然后用cross- attention进行多模态交互，此时视觉编码器和LLM都会被冻结，&lt;strong&gt;只训练Qformer&lt;/strong&gt;，而在下游任务微调时，可以再解锁视觉编码器，让它跟Qformer一起训练&lt;/p&gt;&#xA;&lt;h3 id=&#34;两阶段的训练策略-1&#34;&gt;&#xA;  两阶段的训练策略 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%a4%e9%98%b6%e6%ae%b5%e7%9a%84%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;BLIP-2设计了两阶段的训练策略，以使视觉编码器能学会提取更关键的信息。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;第一阶段：使用多种预训练任务，如Image-Text Contrastive Learning(&lt;strong&gt;ITC&lt;/strong&gt;)，Image-grounded Text Generation(&lt;strong&gt;ITG&lt;/strong&gt;)，Image-Text Matching(&lt;strong&gt;ITM&lt;/strong&gt;)让Qformer学会如何从&lt;strong&gt;视觉编码器中抽取文本相关的特征&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;第二阶段，将Qformer插入到LLMs中，用language modeling进行训练。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;架构3&#34;&gt;&#xA;  架构[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%843&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;两个阶段训练&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;阶段一&#xA;获得高质量的 &lt;strong&gt;图文对齐向量表征&lt;/strong&gt;&#xA;通过&lt;strong&gt;ITC ITM  ITG 三个损失函数&lt;/strong&gt;获得了很好的图片文本 &lt;strong&gt;对齐向量表征能力&lt;/strong&gt;，仅训练&lt;strong&gt;Qformer&lt;/strong&gt;中很少的参数&#xA;【ITM:  image-text 是否是匹配的 |    image 和text 都能相互看到】&#xA;【ITG: image生成text |    image 能全看到, text只能逐个的看】&#xA;【ITC: image和text的对比学习, 对比学习分类分错了的  送入ITM 负样本 |  image和 text  之间是不能看到的】&lt;/li&gt;&#xA;&lt;li&gt;阶段二&#xA;通过向量表征进行&lt;strong&gt;文字生成&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;code-2&#34;&gt;&#xA;  code [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#code-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;blip2&#34;&gt;&#xA;  blip2&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#blip2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://nakaizura.blog.csdn.net/article/details/130757157?spm=1001.2014.3001.5502&#34;&gt;基于LLMs的多模态大模型（Flamingo, BLIP-2，KOSMOS-1，ScienceQA）&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(综述)多模态InstructTuning</title>
      <link>https://www6v.github.io/www6vVision/docs/MLLM/Training/InstructTuning/</link>
      <pubDate>Wed, 15 Mar 2023 16:09:00 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/MLLM/Training/InstructTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;datasets-for-visual-instruction-tuning1&#34;&gt;&#xA;  Datasets for Visual Instruction Tuning[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#datasets-for-visual-instruction-tuning1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;single-turn&#34;&gt;&#xA;  Single-turn&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#single-turn&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MiniGPT-4&#xA;&lt;strong&gt;MiniGPT-4&lt;/strong&gt; [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 &lt;strong&gt;randomly selects 5000 images from the Conceptual Caption dataset&lt;/strong&gt; [38], [39] and prompts its &lt;strong&gt;pre-trained VLM model&lt;/strong&gt; to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)MiniGPT4</title>
      <link>https://www6v.github.io/www6vVision/docs/MLLM/VL/Minigpt4/</link>
      <pubDate>Wed, 15 Mar 2023 15:56:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/MLLM/VL/Minigpt4/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;introduction1&#34;&gt;&#xA;  INTRODUCTION[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;MiniGPT-4 增加了一个&lt;strong&gt;投影层&lt;/strong&gt;，将&lt;strong&gt;编码的视觉特征与 Vicuna 语言模型对齐&lt;/strong&gt;，并&lt;strong&gt;冻结了所有其他视觉和语言组件&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  METHOD[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;图 1&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MiniGPT-4 的目标是将来自预训练视觉编码器的视觉信息与先进的大型语言模型（LLM）对齐（Alignment）。具体来说，&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用 &lt;strong&gt;Vicuna作为语言解码器&lt;/strong&gt;，该解码器基于 LLaMA构建，可以执行各种复杂的语言任务。&lt;/li&gt;&#xA;&lt;li&gt;视觉感知方：采用与 &lt;strong&gt;BLIP-2&lt;/strong&gt; 相同的&lt;strong&gt;视觉编码器&lt;/strong&gt;，&lt;strong&gt;ViT Backbone&lt;/strong&gt;及其预先训练好的 &lt;strong&gt;Q-Former&lt;/strong&gt;。&#xA;语言和视觉模型都是开源的。我们的目标是利用线性投影层弥合视觉编码器与 LLM 之间的差距，图 1 显示了模型概览。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;first-pretraining-stage&#34;&gt;&#xA;  FIRST &lt;strong&gt;PRETRAINING&lt;/strong&gt; STAGE&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#first-pretraining-stage&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;第一阶段：在大量对齐的图像-文本对上对模型进行预训练，以获取视觉语言知识。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Traditional alignment method [2]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Input: Image&lt;/li&gt;&#xA;&lt;li&gt;Output: Caption&lt;/li&gt;&#xA;&lt;li&gt;Training Objective: Maximize the likelihood of GT captions&lt;/li&gt;&#xA;&lt;li&gt;Training Dataset 组合数据集 [postprocessed by BLIP]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Conceptual Caption&lt;/li&gt;&#xA;&lt;li&gt;SBU&lt;/li&gt;&#xA;&lt;li&gt;LAION&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;curating-a-high-quality-alignment-dataset-for-vision-language-domain&#34;&gt;&#xA;  CURATING A &lt;strong&gt;HIGH-QUALITY ALIGNMENT DATASET&lt;/strong&gt; FOR VISION-LANGUAGE DOMAIN.&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#curating-a-high-quality-alignment-dataset-for-vision-language-domain&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create a dataset with detailed, human-perfered descriptions[2][1]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;model  generates descriptions&#xA;在初始阶段，我们使用从第一个预训练阶段得到的模型来&lt;strong&gt;生成输入图像的描述&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;polishing and filtering by chatgpt&#xA;上述自动生成的图片说明包含&lt;strong&gt;噪音或不连贯的描述&lt;/strong&gt;，例如单词或句子重复，句子支离破碎或内容不相关。为了解决这些问题，我们采用了&lt;strong&gt;ChatGPT&lt;/strong&gt;，通过以下提示对描述进行&lt;strong&gt;修补&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;further polishing and filtering by rules &amp;amp; human&#xA;完成后处理阶段后，我们会手动验证每张图片说明的正确性，以保证其高质量。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;second-stage-finetuning&#34;&gt;&#xA;  SECOND-STAGE &lt;strong&gt;FINETUNING&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#second-stage-finetuning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;第二阶段：使用一个较小但高质量的图像-文本数据集对预训练模型进行微调，并设计了对话模板，以提高生成的可靠性和可用性。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;【blip2能识别图像，但是对话能力比较弱，不能说出图像中的细节。在pre-train阶段获取视觉语言知识， 在fine-tuning 阶段获取对话能力】  [2]&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战) LLaVa 演化</title>
      <link>https://www6v.github.io/www6vVision/docs/MLLM/VL/Llava/</link>
      <pubDate>Tue, 14 Mar 2023 23:02:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/MLLM/VL/Llava/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;llava-演化&#34;&gt;&#xA;  LLaVa 演化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llava-%e6%bc%94%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaVa-cef875377c394636a64cf57edbb0026e?pvs=4&#34;&gt;(原理|实战) LLaVa 演化&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;llava-实战&#34;&gt;&#xA;  LLaVa 实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llava-%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaVa-0bf9f127dc7c41e796050bcb8f7fb1b3?pvs=4&#34;&gt;(实战) LLaVa &lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>(原理)多模态预训练 概述</title>
      <link>https://www6v.github.io/www6vVision/docs/MLLM/Training/Pretrain/</link>
      <pubDate>Sat, 04 Mar 2023 13:23:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/MLLM/Training/Pretrain/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;overview&#34;&gt;&#xA;  Overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/overview.png&#34; alt=&#34;overview.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;多模态预训练&#34;&gt;&#xA;  多模态预训练&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81%e9%a2%84%e8%ae%ad%e7%bb%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;数据集&#34;&gt;&#xA;  数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;大规模无标注&lt;/li&gt;&#xA;&lt;li&gt;内容杂  噪音多&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;架构transformer&#34;&gt;&#xA;  架构Transformer&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84transformer&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于transformer encoder-理解任务&#xA;单流 - vl-bert  UNITER&#xA;双流 - ViLBERT， CLIP（双流结构，对比学习）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于transformer decoder-生成任务&#xA;DALL-E  （VQVAE+GPT,  Text-to-Image Generation）&#xA;现在都用 → SD 扩散模型&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于encoder+decoder-理解+生成&#xA;文本的decoder&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;encoder + decoder 串行,  交叉注意力&lt;/li&gt;&#xA;&lt;li&gt;encoder + decoder 并行&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;模型---自监督学习&#34;&gt;&#xA;  模型 - 自监督学习&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a8%a1%e5%9e%8b---%e8%87%aa%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;模态内掩码学习&#xA;文本 语音 视觉自身token级别mask&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
