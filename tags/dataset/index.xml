<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dataset on Vision</title>
    <link>https://www6v.github.io/www6vVision/tags/dataset/</link>
    <description>Recent content in Dataset on Vision</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 01 Apr 2023 15:09:17 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vVision/tags/dataset/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(survey)多模态  数据集</title>
      <link>https://www6v.github.io/www6vVision/docs/Data/MulitmodalDataset/</link>
      <pubDate>Sat, 01 Apr 2023 15:09:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Data/MulitmodalDataset/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;survey0&#34;&gt;&#xA;  Survey[0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Pre-training&lt;/li&gt;&#xA;&lt;li&gt;Adaptation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;pre-training数据集&#34;&gt;&#xA;  Pre-training数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pre-training%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;LAION[1]&#xA;&lt;a href=&#34;https://laion.ai/projects/&#34;&gt;LAION&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;wukong[1]&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/473794131&#34;&gt;[论文]中文多模态数据集WuKong &amp;amp; FILIP &amp;amp; LiT-tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/551622338&#34;&gt;Wukong：一亿规模的中文跨模态预训练基准&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MMDialog&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/584894471&#34;&gt;百万量级的多模态对话数据集来了，153万张图片4000多主题&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;OBELISC[2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ShareGPT4V[3]&#xA;opensource&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;sft数据集&#34;&gt;&#xA;  SFT数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sft%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LAMM&lt;/li&gt;&#xA;&lt;li&gt;MultiIntruct&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;survey&#34;&gt;&#xA;  survey&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/_fi2odhKITs4fs7MbWpWaw&#34;&gt;多模态模型大常用数据集及处理策略：兼看Chatlaw法律问答中的知识图谱融合思路 &lt;/a&gt;&#xA;《A Survey of Multimodal Large Language Model from A Data-centric Perspective》&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;预训练数据集&#34;&gt;&#xA;  预训练数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/686757824&#34;&gt;多模态数据集收集&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/670149958&#34;&gt;[论文阅读] 开源的多模态文档数据集，OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents&lt;/a&gt;&lt;br&gt;&#xA;从网页文档里得到的数据集&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669485001&#34;&gt;超越同级7B模型！ 中国团队开源大规模高质量图文数据集ShareGPT4V，大幅提升多模态性能&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/527182857&#34;&gt;多模态预训练数据集&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
