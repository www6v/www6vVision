[{"id":0,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion/DALLE2vsImagen/","title":"(NT)DALLE-2 vs Imagen","section":"Diffusion","content":" DALLE-2 vs Imagen # 对比维度 DALL-E 2 (OpenAI) Imagen (Google) 关键差异分析 数据来源 技术架构 (Technical Architecture) 基于 unCLIP：包含一个生成 CLIP 图像嵌入的 Prior（先验）和一个将嵌入转换为图像的 Diffusion Decoder（解码器）。【image embedding】 级联扩散模型架构：包含一个冻结的文本编码器和一系列级联的图像扩散模型（64x64, 256x256, 1024x1024）。【text embedding】 DALL-E 2 依赖 CLIP 潜空间作为中间层；Imagen 直接在级联扩散模型中通过文本嵌入进行条件生成，结构更简洁。 [1, 2] 文本编码器类型 (Text Encoder) CLIP 文本编码器（基于图像-文本对预训练）。 冻结的 T5-XXL（大型语言模型，仅在纯文本语料库上预训练）。 Imagen 发现大型语言模型比图像-文本预训练编码器（如 CLIP）在图文一致性上更有效。 [1, 2] 图像生成分辨率 最高 1024 × 1024 (通过两级上采样扩散模型)。 最高 1024 × 1024 (通过级联扩散模型逐步提升分辨率)。 二者均采用多级上采样技术达到百万像素分辨率。 [1, 2] 人类评估表现 (如 DrawBench) 在 DrawBench 上表现弱于 Imagen，尤其在复杂描述和组合性方面。人类更倾向于 Imagen。 在 DrawBench 所有的 11 个类别（包括属性绑定、写实度）中均被评定为优于 DALL-E 2。 Imagen 在图像质量和图文一致性的主观偏好测试中显著领先。 [2] 属性绑定能力 (如颜色/空间关系) 表现较弱；容易混淆对象与属性之间的对应关系（如红色立方体在蓝色立方体上方）。 表现更强；能更好地处理颜色绑定、计数和空间定位描述。 DALL-E 2 的 CLIP 嵌入层可能丢失了细节的属性绑定信息；Imagen 通过大型 LM 捕获了更细致的语义。 [1, 2] 写实度 (Photorealism) 具有极高的写实度，但在处理非常复杂的场景细节时仍有挑战。 被认为具有“前所未有”的写实度，在 DrawBench 的 Fidelity（忠实度）评分中全面胜过 DALL-E 2。 Imagen 利用动态阈值技术在保持写实度的同时支持极高的引导权重。 [2] 扩散模型改进技术 采用 Prior + Decoder 的两阶段扩散；Decoder 通过投影 CLIP 嵌入进行条件化。 动态阈值选择 (Dynamic Thresholding)：允许在高引导权重下防止图像过度饱和；Efficient U-Net 提升了收敛速度和内存效率。 Imagen 的动态阈值是解决高引导权重下写实度下降的关键创新。 [1, 2] 参考 # 基于 2 个来源\n[1] DALLE2-unCLIP.pdf[PPT]\n[2] Imagen(Google).pdf[PPT]\nNotebookLM 提供的内容未必准确，因此请仔细核查回答内容。\n"},{"id":1,"href":"/www6vVision/docs/MLLM/VL/Qwen-VL/Qwen-VL1/","title":"(NT)Qwen-VL","section":"Qwen-VL","content":" "},{"id":2,"href":"/www6vVision/docs/MLLM/VL/Qwen-VL/Qwen-VL2/","title":"(NT)Qwen-VL2","section":"Qwen-VL","content":" "},{"id":3,"href":"/www6vVision/docs/MLLM/VL/Qwen-VL/Qwen-VL3/","title":"(NT)Qwen-VL3","section":"Qwen-VL","content":" "},{"id":4,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion/Diffusion-%E6%80%BB%E7%BB%93/","title":"(原理) Diffusion 总结","section":"Diffusion","content":" Diffusion 总结 # Diffusion 总结\n"},{"id":5,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion/Diffusion%E6%BC%94%E8%BF%9B/","title":"(原理) Diffusion 演进","section":"Diffusion","content":" Diffusion 演进 # Diffusion 演进\n"},{"id":6,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion%E5%9F%BA%E7%A1%80/Diffusion%E5%8E%9F%E7%90%86/","title":"(原理)Diffusion 原理 *","section":"Diffusion 基础","content":" Diffusion 原理 # Diffusion 原理\n"},{"id":7,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion%E5%9F%BA%E7%A1%80/Diffusion/","title":"(原理)Stable Diffusion","section":"Diffusion 基础","content":" Stable Diffusion 原理 # (原理)Stable Diffusion\n"},{"id":8,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion%E5%9F%BA%E7%A1%80/Diffusion%E5%AE%9E%E7%8E%B0/","title":"(实现)Stable Diffusion *","section":"Diffusion 基础","content":" Stable Diffusion 实现 # (实现)Stable Diffusion\n"},{"id":9,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion%E5%9F%BA%E7%A1%80/DiffusionPractice/","title":"(实战)Stable Diffusion","section":"Diffusion 基础","content":" API-based # diffusion-models-class [官方课程] # Unit 1: An Introduction to Diffusion Models Unit 2: Fine-Tuning, Guidance and Conditioning Unit 3: Stable Diffusion Unit 4: Going Further with Diffusion Models diffusers 重点pipeline [10] # controlnet 【controllable】 dreambooth 【fine tuning】 instruct_pix2pix 【image edit】 UI-based # stable-diffusion-webui # stable-diffusion-webui-colab[11] 没试过，colab要充值\nstable-diffusion-webui on 阿里serverless [12]\ncomfyui # ComfyUI on 阿里serverless[13]\n参考 # API-based # Repo diffusers git UI-based # 可白嫖且很香—轻轻松松在colab上部署Stable Diffusion大模型！ V stable-diffusion-webui-colab Repo git Install the WebUI Colab to Google Drive git 运行这3个脚本\n超详细云端部署Stable Diffusion教程！ V 【用FC的应用模版部署】【3个月免费的serverless+NAS】\n函数计算 ComfyUI 使用文档\n用 ComfyUI 自制“粘土滤镜 【用FC的应用模版部署】【3个月免费的serverless+NAS】\n"},{"id":10,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion/DiffusionXL/","title":"(原理)SD XL","section":"Diffusion","content":" SDXL # SDXL\n"},{"id":11,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion%E5%9F%BA%E7%A1%80/Guidance/","title":"(原理)Guidance","section":"Diffusion 基础","content":" Guidance # Guidance\n"},{"id":12,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion/unCLIP/","title":"(原理)unCLIP","section":"Diffusion","content":" unCLIP # (原理)unCLIP\n"},{"id":13,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Diffusion/DiT/","title":"(原理|实战)DiT","section":"Diffusion","content":" 论文 # 论文地址\nScalable Diffusion Models with Transformers\n开源地址 Scalable Diffusion Models with Transformers git\nDiT Repo git\nArch # code[10] # DiT\ndit git DiTBlock\ndit_block git 参考 # 1xx. AI大讲堂：文生视频谁能敌？专业拆解【DiT模型】 V\nDiT原文: https://arxiv.org/abs/2212.09748\nCode: https://github.com/facebookresearch/DiT\nHuggingface: https://huggingface.co/spaces/wpeebles/DiT\n1xx. 14步手搓sora!Diffusion Transformer, DiT工作原理 V\n实战 # 【Sora重要技术】复现DiT（Diffusion Transformer）模型 V ***\ndits Repo "},{"id":14,"href":"/www6vVision/docs/Survey/MultimodalSeries/","title":"多模态 系列","section":"Survey","content":"\n目录 # Stage1: 模块独立[2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nmodel # CLIP ViLT ALBEF Stage2: 模块共享[2] # model # VLMO BLIP BLIP2 BEiTv3 Stage3: 范式统一[2] # model # Unified-IO Uni-Perceiver PaLi 总结 [1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n参考 # Overview # 多模态大模型 CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP 系列解读 ***\n[Transformer 101系列] 多模态的大一统之路 ***\n1xx. 多模态论文串讲 *** 多模态论文串讲：ALBEF \u0026amp; VLMo \u0026amp; BLIP \u0026amp; CoCa \u0026amp; Beit V3\n1xx. 图生文多模态大模型开源项目回顾：兼看20240307大模型进展早报\n1xx. 图文多模态大模型综述\n1xx. Multimodality and Large Multimodal Models (LMMs) 多模态和多模态大模型 (LMM)[译] CLIP Flamingo\n1xx. 写在多模态征服一切之前（未来数据和模型应该是什么样的？）\n"},{"id":15,"href":"/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/VQVAE/","title":"VQ-VAE","section":"Vision Encoder","content":"\n参考 # 关于 VQ-VAE 直观理解\n[论文简析]VQ-VAE:Neural discrete representation learning[1711.00937] v 看评论中的置顶 有代码\n如何搭建VQ-VAE模型（Pytorch代码） v 看视频介绍 https://github.com/KevinOfCathay/DDPM-demo\n"},{"id":16,"href":"/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/DINO/","title":"DINO","section":"Vision Encoder","content":"\nDINO # https://github.com/facebookresearch/dino\n参考 # 重塑自监督学习: DINO 网络如何颠覆视觉特征表示的常规方法 有动图\nDINOv2 # 论文 # 论文地址 DINOv2: Learning Robust Visual Features without Supervision\n开源地址 https://github.com/facebookresearch/dinov2\nProject page Project page Project page\n参考 # DINOv2：无需微调，填补 SAM 的空白，支持多个下游任务\n全网最详细的 DINOv2 论文解读来啦！\n深度学习算法应用实战 | DINOv2 图像相似度实战\n视觉大模型DINOv2:自我监督学习的新领域\nDINOv2：无需微调，填补SAM空白\n实战 # DINOv2 on mmpretrain\n"},{"id":17,"href":"/www6vVision/docs/Vision/VisionTask/","title":"CV 任务","section":"Vision","content":"\n分类 [1] # image-level # image recognition\n(Retrieval)image-text retrieval\nCaption(image captioning)\nVQA(visual question answering)\nregion-level # Object Detection object detection\nDETR -\u0026gt; DINO -\u0026gt; Grounding DINO dense caption\nphrase grounding\npixel-level # Segmentation generic segmetation referring segmetation 其他 # 对比\n[CNN 更深的网络] [transformer 没有局限] CV任务\n分类（Classification） 检测（Detection） 分割（Segmentation） 跟踪（Tracking） 行为识别（Action Recognition） 参考 # [CVPR Tutorial Talk] Towards General Vision Understanding Interface pdf "},{"id":18,"href":"/www6vVision/docs/%E5%A4%9A%E6%A8%A1%E6%80%81-Agent/MultimodalAgentApp/","title":"Agent - UI-assistants","section":"多模态 Agent","content":"\n参考 # App Agent # 1xx. AppAgent源码分析\u0026amp;思考 https://github.com/mnotgod96/AppAgent https://icoz69.github.io/\n1xx. 【LLM-agent】MOBILE-AGENT: 具有视觉感知能力的自治多模移动设备agent https://github.com/X-PLUG/MobileAgent\n1xx. https://github.com/OpenAdaptAI/OpenAdapt\n"},{"id":19,"href":"/www6vVision/docs/Data/MulitmodalDataset/","title":"(survey)多模态  数据集","section":"Data","content":"\n目录 # Survey[0] # Pre-training Adaptation Pre-training数据集 # LAION[1] LAION\nwukong[1] [论文]中文多模态数据集WuKong \u0026amp; FILIP \u0026amp; LiT-tuning Wukong：一亿规模的中文跨模态预训练基准\nMMDialog 百万量级的多模态对话数据集来了，153万张图片4000多主题\nOBELISC[2]\nShareGPT4V[3] opensource\nSFT数据集 # LAMM MultiIntruct 参考 # survey # 多模态模型大常用数据集及处理策略：兼看Chatlaw法律问答中的知识图谱融合思路 《A Survey of Multimodal Large Language Model from A Data-centric Perspective》 预训练数据集 # 多模态数据集收集\n[论文阅读] 开源的多模态文档数据集，OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents\n从网页文档里得到的数据集\n超越同级7B模型！ 中国团队开源大规模高质量图文数据集ShareGPT4V，大幅提升多模态性能 ShareGPT4V git\n1xx. 多模态预训练数据集\n1xx. OpenDataLab\nSFT数据集 # 1xx. 【LMM 015】LAMM：多模态指令微调数据集，框架和基准 1xx. [NeurIPS2023] LAMM：多模态指令微调数据集、框架、评测基准\n1xx. Talk | ACL'23 杰出论文，MultiIntruct：通过多模态指令集微调提升VLM的零样本学习 1xx. 【ACL2023】MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning\n"},{"id":20,"href":"/www6vVision/docs/Survey/MultimodalSurvey/","title":"(Survey)多模态","section":"Survey","content":"\n目录 # 论文 # 论文地址 《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》 .Sep 2023 - microsoft\n开源地址 Computer Vision in the Wild (CVinW)\noverview [0] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n视觉理解 [1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n{% asset_img \u0026rsquo;\u0026rsquo; %}\n视觉生成 [1] # Human Alignments in Visual Generation [10] # 四种alignment的方式\nspatial controllable T2I generation # 结合位置分布的文字描述（比较麻烦的用户交互，不仅需要文字，而且需要位置），常用于对位置要求比较高的创意设计（海报等）\n直接讲原来clip那种image-level的text description升级为基于区域的text description\nreco gligen 将box描述变为spatial condition\ncontrolnet 无需fintinue，直接变为inference-guide\nuniversal guidance for diffusion model text-based editing # 给一张图和对应的修改文字，输出要求的图，常用于ps等产品\ndiffusion process manipulations\npromot2promot text instruction editing\nInstructPix2Pix Editing with external pre-trained models\ntext promots following # 直接给文字描述，生成对应的图，这个是目前常见文生图产品的交互方式，常用于c端或者b端用户图像内容生成。但其对更细节的控制存在一定的难度 Inference-time manipulation StructureDiffusion Attend-and-Excite Model tuning to follow text prompt ddpo concept customization # 给一张图，提取图片中的关键内容，做各种风格（背景/动作）变换，更用于不那么精细的广义产品，比2的运用范围更加广义\nConcept Customization\nTextual Inversion [DreamBooth] Multi-concept customization\nCustom Diffusion Customization without test-time finetuning\nSuTI {% asset_img \u0026rsquo;\u0026rsquo; %}\n{% asset_img \u0026rsquo;\u0026rsquo; %}\nText-to-Image Generation 技术流派（4类） # Generative adversarial networks (GAN) Variational autoencoder (VAE) Discrete image token prediction Diffusion model 统一的视觉模型[2] # 端到端的方式训练LLM[2] # 多模态 Agent[3] # 参考 # 翻译 # 《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》\nAGI之MFM：《Multimodal Foundation Models: From Specialists to General-Purpose Assistants多模态基础模型：从专家到通用助 翻译\nAGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之视觉理解、视觉生成\nAGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之统一的视觉模型、加持LLMs的大型多模态模型\nAGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之与LLM协同工作的多模态智能体、结论和研究趋势\n解读 # 大模型系列04 -文本图像生成 1xx. Multimodal Foundation Models: From Specialists to General-Purpose Assistants\n1xx. 对应第二章节 《Alignments in Text-to-Image Generation》 [CVPR2023 Tutorial Talk] Alignments in Text-to-Image Generation V\n1xx. 对应第三章节 《From Specialist to Generalist: Towards General Vision Understanding Interface》 [CVPR Tutorial Talk] Towards General Vision Understanding Interface\n"},{"id":21,"href":"/www6vVision/docs/MLLM/VL/Blip/","title":"BLIP-2","section":"VL","content":" BLIP-2 # Overview [1] # 用一个Qformer来提取图像特征（等同与Flamingo的perceiver resampler），然后用cross- attention进行多模态交互，此时视觉编码器和LLM都会被冻结，只训练Qformer，而在下游任务微调时，可以再解锁视觉编码器，让它跟Qformer一起训练\n两阶段的训练策略 [1] # BLIP-2设计了两阶段的训练策略，以使视觉编码器能学会提取更关键的信息。\n第一阶段：使用多种预训练任务，如Image-Text Contrastive Learning(ITC)，Image-grounded Text Generation(ITG)，Image-Text Matching(ITM)让Qformer学会如何从视觉编码器中抽取文本相关的特征。 第二阶段，将Qformer插入到LLMs中，用language modeling进行训练。 架构[3] # 两个阶段训练 阶段一 获得高质量的 图文对齐向量表征 通过ITC ITM ITG 三个损失函数获得了很好的图片文本 对齐向量表征能力，仅训练Qformer中很少的参数 【ITM: image-text 是否是匹配的 | image 和text 都能相互看到】 【ITG: image生成text | image 能全看到, text只能逐个的看】 【ITC: image和text的对比学习, 对比学习分类分错了的 送入ITM 负样本 | image和 text 之间是不能看到的】 阶段二 通过向量表征进行文字生成 code [2] # 参考 # blip2 # 基于LLMs的多模态大模型（Flamingo, BLIP-2，KOSMOS-1，ScienceQA）\nblip2 git blip2_instructed_generation git 运行过\n强推！科大讯飞和中科院终于把多模态大模型讲明白了，CLIP、blip、blip2三种模型原理一口气学完 V ***\n1xx. AI论文精读之多模态大模型BLIP-2 V\n1xx. MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践 *\n1xx. BLIP2：下一代多模态模型的雏形\n"},{"id":22,"href":"/www6vVision/docs/MLLM/Training/InstructTuning/","title":"(综述)多模态InstructTuning","section":"Training","content":"\n目录 # Datasets for Visual Instruction Tuning[1] # Single-turn # MiniGPT-4 MiniGPT-4 [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 randomly selects 5000 images from the Conceptual Caption dataset [38], [39] and prompts its pre-trained VLM model to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.\nMultiInstruct MultiInstruct [43] build a comprehensive instruction dataset that covers 62 diverse multimodal tasks from 10 broad categories, such VQA, Image-text matching, grounded generation, and so on. These tasks include 34 existing tasks derived from 21 public dataset and 28 new tasks extended from them. Each task is equipped with 5 instruction templates to prompt the model to perform the specific task.\nMulti-turn # LLaVA LLaVA-Instruct-158k [9] contains 158 image-text instruction data, including 58k conversation data asking about the visual content of the image,23k description data, and 77k complex reasoning data where the question may involve multi-step reasoning process. VLIT Data Construction Strategy[2] # Annotation Adaption # MiniGPT-4 Self-Instruct # LLaVA High-Quality VLIT Data[2] # Correctness # Diversity # Complexity # Method [1][2] # Method Training Paradigm[2] Vision Encoder Language Encoder Inst[2] Tuning Data MiniGPT-4 FA → VLIT EvaCLIP ViT Vicuna AA CC3M, CC12M, SBU, LAION 400M, MiniGPT-3.5K MiniGPT-v2 EVA LLaMA2-chat AA+SI LAION, CC3M, SBU, GRIT-20M, COCO caption, Text Captions, RefCOCO, RefCOCO+, RefCOCOg, GQA, VQA-v2, OCR-VQA, OKVQA, AOK-VQA, Flickr30k Dataset, Unnatural Instruction Dataset LLaVa FA → VLIT CLIP ViT Vicuna SI CC3M Concept-balanced 595K, LLaVA-Instruct-158K LLaVA-1.5 FA → VLIT CLIP ViT Vicuna LLaVA, ShareGPT, VQAv2, GQA, OKVQA, OCRVQA, A-OKVQA, TextCaps, RefCOCO, VG MultiInstruct VLIT OFA OFA AA VQAv2, Visual7w, GQA, OK-VQA, Visual Genome, MSCOCO, RefCOCO, COCO-Text, TDIUC, IQA, VAW, MOCHEG, WikiHow Otter CLIP ViT MPT SI MIMIC-IT LAMM VLIT CLIP ViT-L/14 Vicuna SI Language-Assisted Multi-Modal Instruction-Tuning Dataset Qwen-VL FA → VLIT(Multi-Task Tuning) ViT Qwen-7B LAION-en\u0026amp;zh, DataComp, Coyo, CC12M\u0026amp;3M, SBU, COCO, In-house Data, GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D, GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg, SynthDoG-en\u0026amp;zh, Common Crawl pdf\u0026amp;HTML CogVLM FA → VLIT EVA2-CLIP-E Vicuna-7Bv-1.5 VQAv2, TextVQA StableLLaVA FA → VLIT CLIP-ViT-L/14 LLaMA AA Synthesized Image-Dialogue Dataset Annotation Adaption-\u0026gt; SI # Self-Instruct -\u0026gt; AA # 参考 # 《Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey》 *** 第4 5章 南洋大学\n《Vision-Language Instruction Tuning: A Review and Analysis》 *** 第2 3 4 5章 腾讯\n《Instruction Tuning for Large Language Models: A Survey》 第5章\n"},{"id":23,"href":"/www6vVision/docs/MLLM/VL/Minigpt4/","title":"(原理|实战)MiniGPT4","section":"VL","content":"\n目录 # INTRODUCTION[1] # MiniGPT-4 增加了一个投影层，将编码的视觉特征与 Vicuna 语言模型对齐，并冻结了所有其他视觉和语言组件\nMETHOD[1] # 图 1\nMiniGPT-4 的目标是将来自预训练视觉编码器的视觉信息与先进的大型语言模型（LLM）对齐（Alignment）。具体来说，\n使用 Vicuna作为语言解码器，该解码器基于 LLaMA构建，可以执行各种复杂的语言任务。 视觉感知方：采用与 BLIP-2 相同的视觉编码器，ViT Backbone及其预先训练好的 Q-Former。 语言和视觉模型都是开源的。我们的目标是利用线性投影层弥合视觉编码器与 LLM 之间的差距，图 1 显示了模型概览。 FIRST PRETRAINING STAGE # 第一阶段：在大量对齐的图像-文本对上对模型进行预训练，以获取视觉语言知识。\nTraditional alignment method [2]\nInput: Image Output: Caption Training Objective: Maximize the likelihood of GT captions Training Dataset 组合数据集 [postprocessed by BLIP] Conceptual Caption SBU LAION CURATING A HIGH-QUALITY ALIGNMENT DATASET FOR VISION-LANGUAGE DOMAIN. # Create a dataset with detailed, human-perfered descriptions[2][1] model generates descriptions 在初始阶段，我们使用从第一个预训练阶段得到的模型来生成输入图像的描述。 polishing and filtering by chatgpt 上述自动生成的图片说明包含噪音或不连贯的描述，例如单词或句子重复，句子支离破碎或内容不相关。为了解决这些问题，我们采用了ChatGPT，通过以下提示对描述进行修补 further polishing and filtering by rules \u0026amp; human 完成后处理阶段后，我们会手动验证每张图片说明的正确性，以保证其高质量。 SECOND-STAGE FINETUNING # 第二阶段：使用一个较小但高质量的图像-文本数据集对预训练模型进行微调，并设计了对话模板，以提高生成的可靠性和可用性。 【blip2能识别图像，但是对话能力比较弱，不能说出图像中的细节。在pre-train阶段获取视觉语言知识， 在fine-tuning 阶段获取对话能力】 [2]\n参考 # 原理 # 【LMM 009】MiniGPT-4：使用 Vicuna 增强视觉语言理解能力的多模态大模型 *** MiniGPT-4、表格推理、代码生成、生成式推理-来自斯坦福、北大、阿卜杜拉、达摩院的四位论文一作思辨大模型 V 1xx. miniGPT4：多模态图文理解训练 V 1xx. MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践 1xx. 使用大型语言模型为MiniGPT-4构建视觉语言理解能力 V 实战 # 1xx. 大杀器，多模态大模型MiniGPT-4入坑指南\n"},{"id":24,"href":"/www6vVision/docs/MLLM/VL/Llava/","title":"(原理|实战) LLaVa 演化","section":"VL","content":"\nLLaVa 演化 # (原理|实战) LLaVa 演化 LLaVa 实战 # (实战) LLaVa "},{"id":25,"href":"/www6vVision/docs/%E5%A4%9A%E6%A8%A1%E6%80%81-Agent/WebAgent/","title":"(原理)Web Agent","section":"多模态 Agent","content":"\nweb scenarios [1] # 在网络场景中，代表用户执行特定任务被称为Web导航问题[390]。代理程序解释用户指令，将其分解为多个基本操作，并与计算机进行交互。这通常涉及到填写表单、在线购物和发送电子邮件等网络任务。代理程序需要具备理解复杂网络场景中的指令的能力，适应变化（如嘈杂的文本和动态HTML网页），并推广成功的操作[391]。通过这种方式，代理程序可以在处理未知任务时实现可访问性和自动化[435]，最终使人类免于与计算机用户界面的重复交互。\n通过强化学习训练的代理程序可以有效地模仿人类行为，使用预定义的操作，如键入、搜索、导航到下一页等。它们在基本任务（如在线购物[392]和搜索引擎检索[90]）中表现良好，这些任务已经得到广泛探索。然而，没有语言模型能力的代理程序可能难以适应现实世界互联网中更真实和复杂的场景。在动态、内容丰富的网页上，如在线论坛或在线业务管理[391]，代理程序常常面临性能方面的挑战。\n为了实现代理程序与更真实的网页之间的成功交互，一些研究人员[393；394]开始利用语言模型的强大HTML读取和理解能力。通过设计提示，他们试图使代理程序理解整个HTML源代码，并预测更合理的下一步操作。Mind2Web[389]结合了为HTML进行微调的多个语言模型，使它们能够在现实世界的场景中总结冗长的HTML代码[388]并提取有价值的信息。此外，WebGum[390]通过使用包含HTML截屏的多模态语料库，赋予代理程序视觉感知能力。它同时进行了语言模型和视觉编码器的微调，加深了代理程序对网页的全面理解。\nPerforming specific tasks on behalf of users in a web scenario is known as the web navigation problem [390]. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations [391]. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future [435], ultimately freeing humans from repeated interactions with computer UIs.\nAgents trained through reinforcement learning can effectively mimic human behavior using predefined actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks such as online shopping [392] and search engine retrieval [90], which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or online business management [391], agents often face challenges in performance.\nIn order to enable successful interactions between agents and more realistic web pages, some researchers [393; 394] have started to leverage the powerful HTML reading and understanding abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML source code and predict more reasonable next action steps. Mind2Web [389] combines multiple LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code [388] in real-world scenarios and extract valuable information. Furthermore, WebGum [390] empowers agents with visual perception abilities by employing a multimodal corpus containing HTML screenshots. It simultaneously fine-tunes the LLM and a visual encoder, deepening the agent’s comprehensive understanding of web pages.\npapers [2] # In web scenarios\n[2023/10] OpenAgents: An Open Platform for Language Agents in the Wild. XLang Lab (The University of Hong Kong) arXiv. [paper] [project page] [code] [demo] *** [2023/07] WebArena: A Realistic Web Environment for Building Autonomous Agents. Shuyan Zhou (CMU) et al. arXiv. [paper] [code] * [2023/07] A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. Izzeddin Gur (DeepMind) et al. arXiv. [paper] [2023/06] SYNAPSE: Leveraging Few-Shot Exemplars for Human-Level Computer Control. Longtao Zheng (Nanyang Technological University) et al. arXiv. [paper] [code] * [2023/06] Mind2Web: Towards a Generalist Agent for the Web. Xiang Deng (The Ohio State University) et al. arXiv. [paper] [code] *** [2023/05] Multimodal Web Navigation with Instruction-Finetuned Foundation Models. Hiroki Furuta (The University of Tokyo) et al. arXiv. [paper] [2023/03] Language Models can Solve Computer Tasks. Geunwoo Kim (University of California) et al. arXiv. [paper] [code] [2022/07] WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. Shunyu Yao (Princeton University) et al. arXiv. [paper] [code] * [2021/12] WebGPT: Browser-assisted question-answering with human feedback. Reiichiro Nakano (OpenAI) et al. arXiv. [paper] [2023/05] Agents: An Open-source Framework for Autonomous Language Agents. Wangchunshu Zhou (AIWaves) et al. arXiv. [paper] [code] *** 参考 # 《The Rise and Potential of Large Language Model Based Agents: A Survey》、 The Rise and Potential of Large Language Model Based Agents: A Survey git 1xx. Multimodal Web Navigation with Instruction-Finetuned Foundation Models 1xx. Google DeepMind｜具备规划长程上下文理解和程序合成能力的真实世界WebAgent 1xx. LLMs-Agent 论文: WebAgent, 2023, Izzeddin Gur et al., Google DeepMind. 1xx. OpenAgents web agent # 1xx. WebVoyager：借助强大多模态模型，开创全新的网络智能体 [译]\n"},{"id":26,"href":"/www6vVision/docs/MLLM/Training/Pretrain/","title":"(原理)多模态预训练 概述","section":"Training","content":"\n目录 # Overview # {% asset_img \u0026rsquo;\u0026rsquo; %}\n多模态预训练 # 数据集 # 大规模无标注 内容杂 噪音多 架构Transformer # 基于transformer encoder-理解任务 单流 - vl-bert UNITER 双流 - ViLBERT， CLIP（双流结构，对比学习）\n基于transformer decoder-生成任务 DALL-E （VQVAE+GPT, Text-to-Image Generation） 现在都用 → SD 扩散模型\n基于encoder+decoder-理解+生成 文本的decoder\nencoder + decoder 串行, 交叉注意力 encoder + decoder 并行 模型 - 自监督学习 # 模态内掩码学习 文本 语音 视觉自身token级别mask\n模态间掩码学习 不同模态信息的相互预测 mask视觉， 输出对应文本\n模态间匹配学习 匹配与否的分类问题 - 正负样本(二分类) 对比学习 - 模态间的图文匹配对\n下游任务 # 多模态下游任务-模型微调 # 模型微调\np+ finetune（全参数） p+ prompt-tuning p+ adaptor-tuning p+ lora 多模态下游任务\n理解： text/audio/visual 内容生成 生成： 跨模态 检索/问答/推理 更大更强的多模态预训练模型 # 强大的语言模型 更大的视觉模型 更大规模的预训练数据 更多模态形式的数据 参考 # 中科院刘静：多模态预训练的进展回顾与展望 V "},{"id":27,"href":"/www6vVision/docs/%E5%A4%9A%E6%A8%A1%E6%80%81-Agent/MultimodalAgent/","title":"(原理)Agent 多模态","section":"多模态 Agent","content":"\n目录 # 论文 # 论文地址 《Large Multimodal Agents: A Survey》\n开源地址 Repo git\nSurvey # 类型 I：无长期记忆的闭源 LLMs 作为规划器。 # Visual ChatGPT ***\nMM-REACT ***\nViperGPT\nHuggingGPT ***\nChameleon ***\nLLaVA-Interactive ***\nSeeAct\nGPT-Driver\nMobile-Agent\n类型 II：无长期记忆的微调 LLMs 作为规划器。 # LLaVA-Plus\nGPT4Tools\n类型 IV：具有本地长期记忆的规划器。 # JARV IS-1\nAppAgent\nDLAH\n多模态 Agent[1] # 核心组件\n感知组件关注处理多模态信息 规划器负责推理和制定计划 行动组件执行计划 记忆组件则涉及长期和短期记忆 四种类型\n无长期记忆的闭源 LLMs 作为规划器 无长期记忆的微调 LLMs 作为规划器 具有间接长期记忆的规划器 具有本地长期记忆的规划器 多智能体协作\n讨论了 LMAs 如何通过协作框架共同实现共同目标。 多模态 Agent[10] # 范式 # {% asset_img \u0026rsquo;\u0026rsquo; %}\nMM-ReAct\nHuggingGPT[21, 22]\nChameleon\nVisual ChatGPT [20]\nworks # {% asset_img \u0026rsquo;\u0026rsquo; %}\n参考 # 综述 # 2024年大型多模态智能体(Large Multimodal Agents)综述：组件, 分类，协作，评估，应用，展望 *** 1xx. 智体AI在多模态交互领域的综述（上） 1xx. 智体AI在多模态交互领域的综述（下）\nxxx # 多模态 Agents：用大模型语言模型串联多模态专家 V 多模态Agent # 1xx. {% post_link \u0026lsquo;gptMultimodal\u0026rsquo; %} self 1xx. {% post_link \u0026lsquo;gptMultimodalSurvey\u0026rsquo; %} self\nxxx # 《Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models》 Visual ChatGPT git\nLLMs的自动化工具系统（HuggingGPT、AutoGPT、WebGPT、WebCPM）\nHuggingGPT git hugginggpt in langchain git langchain-huggingGPT git\n1xx. Visual Programming——实现通用人工智能的另一种方式 2022 best paper\n"},{"id":28,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/FineTuning/","title":"(work|实战) fine-tuning","section":"Controllable","content":" 对比总结[1] # 训练方法 方法 局限性 Text Inversion 使用提供的一组图片训练一个新单词的Embedding ，并将其与词汇表中的已有单词关联起来，这个新单词即为这组图片概念的指代。 训练过程只对应 Embedding，扩散模型没有新知识输入，所以也无法产生新的内容。 Full FineTune 最朴素的方式，使用图片+ 标注的数据集，进行迭代训练，数据集标注可以选择BLIP来生成。训练直接对原模型的所有权重进行调整。 容易过拟合，导致生成图片的多样性不够，结果难以控制。模型体积大，不便于传播。 Dreambooth 提供代表某个新概念（instance） 对应的一组图像，并使用罕见字符（identifier） 进行概念Mapping，训练过程充分考虑原有相关主题（class）生成，避免过拟合。训练直接对原模型的所有权重进行调整。 训练过程只针对新概念 （instance），多样性差。如果需要多概念生成，需要多次训练。模型体积大，不便于传播。 LoRA（w Dreambooth） 冻结预训练模型参数，在每个Transformer块插入可训练层，不需要完整调整 UNet 模型的全部参数。训练结果只保留新增的网络层，模型体积小。 训练效果不如Dreambooth 对比总结[2] # 对比总结[3] # 参考 # Stable Diffusion 微调及推理优化 【论文串读】Stable Diffusion模型微调方法串读 V Stable Diffusion——四种模型 LoRA（包括LyCORIS）、Embeddings、Dreambooth、Hypernetwork 实战 # 1xx. Text Inversion V 【这个比较详细】 1xx. lora Dreambooth V 【冻结不训练unet，只训练lora】 【为unet模型添加注意力层，注意力层是要训练的参数】 【大部分代码和Dreambooth差不多】\n实战 # 1xx. + concept customization dreambooth lora + textual_inversion diffusers\n1xx. 手把手教你微调Stable Diffusion * lora on DreamBooth\n"},{"id":29,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/ControllableWork/","title":"(Work|实战)Controllable","section":"Controllable","content":" 实战 # ControlNet + t2i_adapter diffusers Custom Diffusion diffusers 总结[metaso] # 功能定位 性能与效率 应用场景 总结 ControlNet 主要用于对图像生成过程中的特定部分进行控制 较大且可能需要较多计算资源 适用于需要对图像特定区域进行精细控制的场景 【ControlNet 结构控制， image2image】 T2I-Adapter 专注于将文本提示转换为图像 较小且更高效，适合资源受限的环境 适用于需要从文本描述生成图像的场景 【T2I-Adapter 多种控制, text2image】 IP-Adapter 用于分析图像提示并提取特征，再将其用于图像生成 在图像质量和对齐方面表现优异 适用于需要结合图像和文本提示进行复杂图像生成的场景 【IP-Adapter 风格特征控制, text2image|image2image 】 "},{"id":30,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEditWork/","title":"(Work|实战)Image Editing","section":"Editing","content":" 总结 # Prompt-to-Prompt\ntrain-free，察觉到了attention map的妙用\npix2pix-zero\n察觉到了attention map的妙用\nInstructPix2Pix\ntrainable，training数据基于Prompt-to-Prompt\nMGIE\n基于LMM\n"},{"id":31,"href":"/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/CLIP/","title":"(原理)CLIP","section":"Vision Encoder","content":" CLIP 在训练过程中做了哪些事情？[Elmo][1] # 在训练过程中，CLIP（Contrastive Language-Image Pre-training）模型主要进行了以下几个步骤：\n数据预处理 : CLIP 使用了一个大规模的数据集，包含 4 亿个 “图像 - 文本” 对，这些数据需要进行清洗和预处理，以便于模型学习。 特征提取 : 通过 Text Encoder 和 Image Encoder 分别对文本和图像进行特征提取。Text Encoder 通常是基于 Transformer 的模型，而 Image Encoder 可以是基于 CNN（卷积神经网络）或者 VIT（Vision Transformer）的模型。 对比学习 : CLIP 采用对比学习的策略，通过对比正确的图像 - 文本对与错误的图像 - 文本对，使得模型能够学习到正确对的特征表示与其他对的区分。具体来说，CLIP 通过 InfoNCE 损失函数来最大化正确对的相似度，同时最小化错误对的相似度。 特征空间对齐 : 通过对比学习，CLIP 将图像和文本的特征映射到一个共享的多模态特征空间中，使得图像特征和文本特征可以直接进行相似度比较。 参数优化 : 通过反向传播和梯度下降等方法，不断调整模型参数，以最小化损失函数，从而优化模型性能。 Zero-shot 推理能力的培养 : 在训练过程中，CLIP 学习了如何通过文本提示（prompts）来进行 zero-shot 的图像分类，即在没有直接观测到的类别标签下，通过文本描述来识别图像内容。 模型评估与调整 : 通过在验证集上的评估，调整模型结构和超参数，以提高模型的泛化能力和性能。 通过这些训练步骤，CLIP 能够学习到一个强大的多模态特征表示，使其能够在多种视觉任务上进行 zero-shot 或 few-shot 的推理。\nCLIP Zero-shot推理[1] # 步骤 # 首先，我们创建一个标签全集，如图中（2）所示，并得到每一个标签的特征向量 然后，我们取一张图片，如图中（3）所示，过Image Encoder后得到该图片的特征向量 最后，计算图片向量和文字向量间的相似度，取相似度最高的那条label即可。 局限 # 当你喂给CLIP一张图时，不管这张图片它是否有见过，CLIP都不会生成一个全新的标签，而是去全集标签中找一个最相似的给你。\n参考 # 关于多模态经典之作CLIP，还有哪些细节是你不知道的 代码 1xx. 神器CLIP：连接文本和图像，打造可迁移的视觉模型 ***\n1xx. 【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision ***\n1xx. CLIP 模型解读\n1xx. 详解CLIP (一) | 打通文本-图像预训练实现ImageNet的zero-shot分类，比肩全监督训练的ResNet50/101\n1xx. openai多模态大模型：clip详解及实战\n1xx. CLIP：多模态领域革命者\n"},{"id":32,"href":"/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Connector/Connector/","title":"(原理)Connector","section":"Connector","content":" Input Projector[11] Connector[10] # Input Projector输入投影器 Cross-attention Flamingo, Owl, Qwen-VL Q-Former BLIP2, InstructBLIP, MiniGPT-4, MiniGPT-5 MLP[10] CogVLM , LLaVa1.5 Linear Project LLaVa, PaLI-x, MiniGPT-v2 Perceiver resampler[10] Flamingo CNN [10] DocOwl 1.5[H-Reducer?] MLP[1] # 定义 # 多层感知机（MLP，Multi-Layer Perceptron）属于前馈神经网络（Feedforward Neural Network）的一种。在模型 训练过程中，需要通过反向传播算法计算梯度，将误差从输出层反向传播回输入层，用于更新网络参数。它包含至少三层节点：一个输入层，一个或多个隐藏层，以及一个输出层。每一层的节点都全连接到下一层的每个节点。MLP 模型通常用于解决分类和回归问题。\nPyTorch代码 [2] # class MLP(nn.Module): # 继承自 nn.Module，这是所有 PyTorch 模型的基类。 def __init__(self, vocab_size, context_length, embedding_size, hidden_size, rng): # 接受五个参数：vocab_size（词汇表大小）、context_length（上下文长度）、 ## embedding_size（嵌入层大小）、hidden_size（隐藏层大小）和 rng（随机数生成器）。 # 调用 super().__init__() 初始化父类 nn.Module。 super().__init__() # 定义一个嵌入层 self.wte，使用 nn.Embedding 将输入的 token 索引转换为嵌入向量。 ## vocab_size 是词汇表的大小，embedding_size 是嵌入向量的维度。 self.wte = nn.Embedding(vocab_size, embedding_size) # 使用 nn.Sequential 定义一个多层感知机（MLP）： # self.mlp = nn.Sequential( nn.Linear(context_length * embedding_size, hidden_size), # 第一层全连接层，将输入的上下文嵌入向量映射到隐藏层。 nn.Tanh(), # # Tanh 激活函数。 nn.Linear(hidden_size, vocab_size) # 第二层线性层，将隐藏层的输出映射到词汇表大小的输出。 ) self.reinit(rng) # 调用 reinit 函数，使用自定义的随机数生成器 rng 初始化权重。 @torch.no_grad() def reinit(self, rng): # 定义 reinit 函数，并使用 @torch.no_grad() 装饰器，表示在这个函数中不需要计算梯度。 def reinit_tensor_randn(w, mu, sigma): # 以正态分布 N(mu, sigma) 初始化张量 w 的权重。 winit = torch.tensor(rng.randn(w.numel(), mu=mu, sigma=sigma)) w.copy_(winit.view_as(w)) def reinit_tensor_rand(w, a, b): # 以均匀分布 U(a, b) 初始化张量 w 的权重。 winit = torch.tensor(rng.rand(w.numel(), a=a, b=b)) w.copy_(winit.view_as(w)) # Let\u0026#39;s match the PyTorch default initialization: # 以均值为0、标准差为1的正态分布初始化嵌入层 self.wte 的权重。 reinit_tensor_randn(self.wte.weight, mu=0, sigma=1.0) scale = (self.mlp[0].in_features)**-0.5 # 算第一层全连接层的缩放系数 scale，其值为输入特征数量的负平方根。 # 以均匀分布 U(-scale, scale) 初始化第一层全连接的权重和偏置。 reinit_tensor_rand(self.mlp[0].weight, -scale, scale) reinit_tensor_rand(self.mlp[0].bias, -scale, scale) # 对于第二层全连接层的处理同上 scale = (self.mlp[2].in_features)**-0.5 reinit_tensor_rand(self.mlp[2].weight, -scale, scale) reinit_tensor_rand(self.mlp[2].bias, -scale, scale) def forward(self, idx, targets=None): # 与 MLPRaw 类的 forward 函数基本相同，但更简洁。 B, T = idx.size() emb = self.wte(idx) # (B, T, embedding_size) emb = emb.view(B, -1) # (B, T * embedding_size) logits = self.mlp(emb) loss = None if targets is not None: loss = F.cross_entropy(logits, targets) return logits, loss 参考 # LLM101n 硬核代码解读：超详解读numpy实现多层感知机MLP LLM101n 硬核代码解读：手把手教你用PyTorch实现多层感知机MLP Self\n《谢春宇_多模态大模型：开放世界理解》\n+ MM-LLMs: Recent Advances in MultiModal Large Language Models\n"},{"id":33,"href":"/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/seg/SAM/","title":"(原理|实战) SAM","section":"Segmentation","content":" 论文[1] # 论文地址 Segment Anything Model论文\n开源地址 Segment Anything Model模型源码 Git\n官网 Segment Anything Model官网\nSegment Anything Model官网demo网页端\n模型[2] # image encoder # prompt encoder # mask decoder # 在prompt embeddings中插入一个可学习的token，用于docoder的输出。 （1）prompt toekns+output tokens进行self attn, （2）用得到的token和image embedding进行 cross attn（token作为Q） （3）point-wise MLP 更新token （4）用image embedding和（3）的token进行cross atten（image embedding作为Q） 重复上述步骤2次，再将attn再通过残差进行连接，最终输出masks和iou scores。\nSAM应用[3] # 图像分割 目标检测 图像修复( image inpainting) 模型微调 参考 # 【模型解读】【代码复现】Segment Anything Model(SAM) * 【论文解读】MetaAi SAM(Segment Anything) 分割一切 *** 最下面有应用 Segment Anything(sam)项目整理汇总 *** 1xx. 【Segment Anything 模型深度解构】GPT时代，干翻计算机视觉第一步！\n"},{"id":34,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/ControlNet/","title":"(原理|实战)ControlNet","section":"Controllable","content":" 论文[ControlNet] # 论文地址 Adding Conditional Control to Text-to-Image Diffusion Models 开源地址 ControlNet git ControlNet[10] # ControlNet is a type of model for controlling image diffusion models by conditioning the model with an additional input image. There are many types of conditioning inputs (canny edge, user sketching, human pose, depth, and more) you can use to control a diffusion model. This is hugely useful because it affords you greater control over image generation, making it easier to generate specific images without experimenting with different text prompts or denoising values as much.\nControlNet是一种模型，用于通过使用额外的输入图像调节模型来控制图像扩散模型。您可以使用**多种类型的调节输入（精巧的边缘、用户草图、人体姿势、深度等）**来控制扩散模型。这非常有用，因为它为您提供了对图像生成的更大控制，从而可以更轻松地生成特定图像，而无需尝试使用不同的文本提示或对值进行过多的去噪。\nMethod [2][3] # ControlNet 采用了一种类似微调的方法，如下图，在原模型的基础上，增加一个可训练副本，可训练副本的输入是原输入x加上条件c，然后把两个模型的输出相加，可训练副本的输入输出都经过零卷积(zero convolution)处理，用于在刚开始训练时保持模型的稳定性。\n{% asset_img \u0026rsquo;\u0026rsquo; %}\n具体的针对 Stable Diffusion 的 ControlNet 结构如下图，只复制了 UNet 的 Encoder blocks 和 Middle block (结构+权重)，控制条件图c先经过几层卷积，再与原UNet的输入zt相加作为输入，ControlNet 每个 block 的输出再 Add 到原 UNet 的 Decoder block 输入，实际实现中，ControlNet 的输出还可以乘上一个scale，用于控制影响程度。注意这里 ControlNet 同样输入了和原 UNet 一样的 Prompt\u0026amp;Time\n{% asset_img \u0026rsquo;\u0026rsquo; %}\n完整的 Diffusion + ControlNet 流程如下：\n{% asset_img \u0026rsquo;\u0026rsquo; %}\nConditional Inputs[2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nComparison with T2I-Adapter[2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nT2I-Adapter vs. Controlnet # Complexity 更高【Conditional Input可以叠加到一起】\nFlexibility 更高 【每种输入有一种Adaptor，多种Adaptor可以配合使用】\n参考 # 【AI论文精读】【图像生成】全网最详细controlnet论文逐段精读 V 【2023 ControlNet】斯坦福最新的可控文本生成图像扩散模型 V ControlNet 算法原理与代码解释 *** 优化，代码 1xx. 深入浅出完整解析ControlNet核心基础知识 ***\n不得不读 | 深入浅出ControlNet，一种基于生成扩散模型Stable Diffusion、可控生成的AIGC绘画生成算法！\n实践 # ControlNet Hugggingface control net V "},{"id":35,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/DreamBooth/","title":"(原理|实战)DreamBooth","section":"Controllable","content":" Approach[1] # Our method takes as input a few images (typically 3-5 images suffice, based on our experiments) of a subject (e.g., a specific dog) and the corresponding class name (e.g. \u0026ldquo;dog\u0026rdquo;), and returns a fine-tuned/\u0026ldquo;personalized\u0026rsquo;\u0026rsquo; text-to-image model that encodes a unique identifier that refers to the subject. Then, at inference, we can implant the unique identifier in different sentences to synthesize the subjects in difference contexts. 我们的方法将主题（例如，特定的狗）和相应的类名称（例如“狗”）的一些图像（根据我们的实验，通常 3-5 个图像就足够了）作为输入，并返回一个微调/ “个性化”文本到图像模型，编码指向主题的唯一标识符。然后，在推理时，我们可以将唯一标识符植入不同的句子中，以合成不同上下文中的主题。\nGiven ~3-5 images of a subject we fine tune a text-to-image diffusion in** two steps**:(a) fine tuning the low-resolution text-to-image model with the input images paired with a text prompt containing a unique identifier and the name of the class the subject belongs to (e.g., \u0026ldquo;A photo of a [T] dog”), in parallel, we apply a class-specific prior preservation loss, which leverages the semantic prior that the model has on the class and encourages it to generate diverse instances belong to the subject\u0026rsquo;s class by injecting the class name in the text prompt (e.g., \u0026ldquo;A photo of a dog”). (b) fine-tuning the super resolution components with pairs of low-resolution and high-resolution images taken from our input images set, which enables us to maintain high-fidelity to small details of the subject. 给定约 3-5 个主题的图像，我们分两步微调文本到图像的扩散：(a) 使用与包含唯一的文本提示配对的输入图像微调低分辨率文本到图像模型标识符和主题所属类的名称（例如，“[T]狗的照片”），同时，我们应用特定于类的先验保存损失，它利用模型在类并鼓励它通过在文本提示中注入类名（例如“狗的照片”）来生成属于主题类的不同实例。 (b) 使用从输入图像集中获取的低分辨率和高分辨率图像对来微调超分辨率组件，这使我们能够保持对象小细节的高保真度。\nDreambooth实战 # Dreambooth in Diffusers [10] # ### download pic from huggingface_hub import snapshot_download local_dir = \u0026#34;./dog\u0026#34; snapshot_download( \u0026#34;diffusers/dog-example\u0026#34;, local_dir=local_dir, repo_type=\u0026#34;dataset\u0026#34;, ignore_patterns=\u0026#34;.gitattributes\u0026#34;, ) ### training ### 在modelscope上运行有问题，连不上huggingface export MODEL_NAME=\u0026#34;CompVis/stable-diffusion-v1-4\u0026#34; export INSTANCE_DIR=\u0026#34;dog\u0026#34; export OUTPUT_DIR=\u0026#34;path-to-save-model\u0026#34; accelerate launch train_dreambooth.py \\\\ --pretrained_model_name_or_path=$MODEL_NAME \\\\ --instance_data_dir=$INSTANCE_DIR \\\\ --output_dir=$OUTPUT_DIR \\\\ --instance_prompt=\u0026#34;a photo of sks dog\u0026#34; \\\\ --resolution=512 \\\\ --train_batch_size=1 \\\\ --gradient_accumulation_steps=1 \\\\ --learning_rate=5e-6 \\\\ --lr_scheduler=\u0026#34;constant\u0026#34; \\\\ --lr_warmup_steps=0 \\\\ --max_train_steps=400 \\\\ --push_to_hub # examples/dreambooth/train_dreambooth.py ### 把prior loss加到instance loss上 if args.with_prior_preservation: # Add the prior loss to the instance loss. loss = loss + args.prior_loss_weight * prior_loss dreambooth in Diffusers examples[11] # if args.with_prior_preservation: # Chunk the noise and noise_pred into two parts and compute the loss on each part separately. noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0) target, target_prior = torch.chunk(target, 2, dim=0) # Compute instance loss loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\u0026#34;none\u0026#34;).mean([1, 2, 3]).mean() # Compute prior loss prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\u0026#34;mean\u0026#34;) # Add the prior loss to the instance loss. loss = loss + args.prior_loss_weight * prior_loss else: loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\u0026#34;mean\u0026#34;) accelerator.backward(loss) 参考 # Dreambooth 原理 # DreamBooth Repo 1xx. DreamBooth 论文精读+通俗理解【论文精读】Dreambooth：diffusion生成模型微调方法 V\nDreambooth 实战 # 使用 Diffusers 通过 Dreambooth 技术来训练 Stable Diffusion\nDreambooth Repo\ndreambooth Diffusers examples on Colab 运行过 Initial setup Settings for teaching your new concept Teach the model the new concept (fine-tuning with Dreambooth) Run the code with your newly trained model\n1xx. + Unit 3: Stable Diffusion\n1xx. Dreambooth V 【只训练unet】\n"},{"id":36,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/IPAdapter/","title":"(原理|实战)IP-Adapter","section":"Controllable","content":" 论文[IP-Adapter] # 论文地址 IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models 开源地址 IP-Adapter git enable a pretrained text-to-image diffusion model to generate images with image prompt 有很多notebook的demo Project page Project page IP-Adapter[10] # IP-Adapter is an image prompt adapter that can be plugged into diffusion models to enable image prompting without any changes to the underlying model. Furthermore, this adapter can be reused with other models finetuned from the same base model and it can be combined with other adapters like ControlNet. The key idea behind IP-Adapter is the decoupled cross-attention mechanism which adds a separate cross-attention layer just for image features instead of using the same cross-attention layer for both text and image features. This allows the model to learn more image-specific features.\nIP-Adapter 是一种图像提示适配器，可以插入扩散模型以启用图像提示，而无需对底层模型进行任何更改。此外，该适配器可以与从同一基本模型微调的其他模型重复使用，并且可以与其他适配器（如 ControlNet）结合使用。IP-Adapter背后的关键思想是解耦的交叉注意力机制，该机制仅为图像特征添加了一个单独的交叉注意力层，而不是对文本和图像特征使用相同的交叉注意力层。这使模型能够学习更多特定于图像的特征。\nMethod[2] # 解耦注意力机制[1]\nIPAdapter提出的解耦注意力机制decoupled cross attention是指将图像提示（image prompt）也看作是text，按照上面的公式原理计算image prompt和查询特征 的cross attention，最后将text prompt计算的cross attention和image prompt计算的cross attention相加。\n参考 # IPAdapter原理和代码解析 【腾讯】IP-Adapter论文解读，拿捏图生图，人脸更不在话下。 v 实战 # IP-Adapter diffusers 1xx. ip-adaptor on colab with diffusers 运行过没异常\n1xx. IPAdapter 多个图像风格混合到一张图像上 comfyUI工作流 v 有代码\n"},{"id":37,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/ReferenceNet/","title":"(原理|实战)ReferenceNet","section":"Controllable","content":" ReferenceNet # (原理|实战)ReferenceNet\n"},{"id":38,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/T2IAdapter/","title":"(原理|实战)T2I-Adapter","section":"Controllable","content":" 论文[T2I-Adapter] # 论文地址 T2I-Adapter 开源地址 T2I-Adapter git T2I-Adapter[10] # T2I-Adapter is a lightweight adapter for controlling and providing more accurate structure guidance for text-to-image models. It works by learning an alignment between the internal knowledge of the text-to-image model and an external control signal, such as edge detection or depth estimation.\nT2I-Adapter 是一种轻量级适配器，用于控制文本到图像模型并提供更准确的结构指导。它的工作原理是学习文本到图像模型的内部知识与外部控制信号（如边缘检测或深度估计）之间的对齐。\nThe T2I-Adapter design is simple, the condition is passed to four feature extraction blocks and three downsample blocks. This makes it fast and easy to train different adapters for different conditions which can be plugged into the text-to-image model. T2I-Adapter is similar to ControlNet except it is smaller (~77M parameters) and faster because it only runs once during the diffusion process. The downside is that performance may be slightly worse than ControlNet.\nT2I-Adapter的设计很简单，条件被传递给四个特征提取模块和三个下采样模块。这使得针对不同条件训练不同的适配器变得快速而容易，这些适配器可以插入到文本到图像模型中。T2I-Adapter 类似于 ControlNet，不同之处在于它更小（~77M 参数）和更快，因为它在扩散过程中只运行一次。缺点是性能可能比 ControlNet 稍差。\nMotivation[1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nMethod[1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n参考 # 1.【北大-腾讯最新工作】T2I-Adapter 更加可控的文本生成图像 V\n1xx. T2I-Adapter：挖掘更多SD模型的控制能力\n1xx. Efficient Controllable Generation for SDXL with T2I-Adapters\n实践 # T2I-Adapter hugggingface "},{"id":39,"href":"/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/ViT/","title":"(原理|实战)ViT, ViLT","section":"Vision Encoder","content":" 论文 # 论文地址 AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n开源地址 vision_transformer git\nmodel # 代码[1] # 参考 # ViT # 【Sora重要技术】复现ViT（Vision Transformer）模型 V mnist-vit Repo git 1xx. VIT (Vision Transformer) 模型论文+代码(源码)从零详细解读，看不懂来打我 V\n1xx. 详解VIT（Vision Transformer)模型原理, 代码级讲解 VIT Repo git ***\n1xx. ViT｜ Vision Transformer ｜理论 + 代码 V PPT\nViLT # 1xx. ViLT：最简单的多模态Transformer 1xx. ViLT git 1xx. ViLT 论文精读【论文精读】 ViLT 论文精读【论文精读】 V 1xx. 多模态ViLT模型下游任务微调原理及代码\n"},{"id":40,"href":"/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/CLIPPractice/","title":"(实战)CLIP","section":"Vision Encoder","content":" CLIP Training [9] # # image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality # ------------------------------------------------- # 1、图像/文字数据过image/text encoder，提取单模态特征 # 每张图片对应一个基本特征I_i # 每张文字对应一个基本特征T_i # ------------------------------------------------- I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # ------------------------------------------------- # 2. 图像/文字的基本特征过多模态Embedding，提取多模态特征 # 同时对这两个多模态特征做Layer Norm # ------------------------------------------------- I_e = l2_normalize(np.dot(I_f, W_i), axis=1) # [n, d_i] * [d_i, d_e] = [n, d_e] T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # [n, d_t] * [d_t, d_e] = [n, d_e] # ------------------------------------------------- # 3、计算图片-文字向量的余弦相似度 # ------------------------------------------------- logits = np.dot(I_e, T_e.T) * np.exp(t) # [n, n] # ------------------------------------------------- # 4、计算Loss # ------------------------------------------------- labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2 CLIP分为按行计算Loss和按列计算Loss 按行计算Loss，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字。 按列计算Loss，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片。 最后将这两个Loss相加取平均，代表我们在模型优化过程中考虑了“图片-\u0026gt;文字”和“文字-\u0026gt;图片”的双向关系。 Simple Demo[10] # 【基于clip on resnet, 数据集为mnist中的\u0026lt;数字文本，数字图片\u0026gt;对】\nopen_clip[11] # Training CLIP python -m training.main \\ --save-frequency 1 \\ --zeroshot-frequency 1 \\ --report-to tensorboard \\ --train-data=\u0026#34;/path/to/train_data.csv\u0026#34; \\ # 训练数据 --val-data=\u0026#34;/path/to/validation_data.csv\u0026#34; \\ # 验证数据 --csv-img-key filepath \\ --csv-caption-key title \\ --imagenet-val=/path/to/imagenet/root/val/ \\ --warmup 10000 \\ # --batch-size=128 \\ # --lr=1e-3 \\ # --wd=0.1 \\ --epochs=30 \\ # --workers=8 \\ --model RN50 # 模型 Chinese-CLIP # 方法[20] # 我们的核心方法在于把训练分为两阶段（如上图所示），第一阶段和LiT是一致的，冻结图像塔，让文本塔表示接近图像塔表示。当训练继续但下游精度不能再产生显著提升，即下游零样本检索的精度，我们就把训练切换到第二阶段，即解除图像塔的参数冻结，继续用contrastive tuning预训练，同样直到下游精度没有显著提升。后者的意义在于让图像塔能拟合中文世界的图像数据的分布，学习中文世界的知识。更多实验参数欢迎查看论文的附录部分。\ndemo[21] # 代码都看过\n# 图片库特征抽取代码 python3 extract_embeddings.py # 图片特征在faiss向量数据库建立索引 python3 build_index.py # 可视化应用界面 streamlit run app.py 参考 # 实战 # 关于多模态经典之作CLIP，还有哪些细节是你不知道的\n【多模态】复现OpenAI的CLIP模型 V mnist-clip Repo git\nopen_clip Repo git Interacting with open_clip\nChinese-CLIP # 中文CLIP模型卷土重来，这次加量不加价！ 论文\nAIGC之图片生成——基于clip内容检索 clip_retrieval git\ndemos Repo readme有解释\n1xx. 【已重新开源】CLIP的中文副本？说不定有惊喜呢\n1xx. Chinese-CLIP Repo git\n1xx. 中文CLIP文到图搜索应用 demo\nxxx # 1xx. langchain 中有CLIP的实现\n1xx. GitHub - jina-ai/clip-as-service: Scalable embedding, reasoning, ranking for images and sentences with CLIP git\n"},{"id":41,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/Controllable/","title":"(综述)Controllable","section":"Controllable","content":" 论文 # 论文地址 A Survey of Multimodal Controllable Diffusion Models Overview # Formulation # Semantic Control # text-to-image[79, 82–84]\nT2I-Adapter[84]\nSpatial Control # Layout- or segmentation-guided approaches[83, 87–91]\nSketch- or edge-guided approaches[22, 84, 92–96] ControlNet[22] T2I-Adapter[84]\nDepth-guided approaches[22, 84, 94–97] ControlNet[22] T2I-Adapter[84]\nSkeleton-guided approaches[22, 27, 84, 93, 95, 96] ControlNet[22] T2I-Adapter[84]\nID Control # Paint by example[104] edit\nStyle Control # Textual Inversion[111], Dreambooth[112], or LoRA[113]\nControllability Trade-Off # Methodologies # Guidance # Controlnet classifier-guidance Condition # GLIDE classifier-free guidance (CFG) Attention-Based Modification【cross-attention】 # Prompt-to-prompt Zero-shot image-to-image translation "},{"id":42,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEdit/","title":"(综述)Image Editing","section":"Editing","content":" 论文 # 论文地址 《Diffusion Model-Based Image Editing: A Survey》\n开源地址 Repo git\n图像编辑[1] # 大类 # 从图片编辑的任务方面可以被分为3个大类 语义编辑semantic editing 风格编辑stylistic editing 结构编辑structural editing APPROACHES # TRAINING-BASED APPROACHES\nInstructPix2Pix TESTING-TIME FINETUNING APPROACHES\nTRAINING AND FINETUNING FREE APPROACHES\n论文[2] # 论文地址 《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 复旦、南洋理工\n开源地址 A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models\nSurvey[2] # 参考 # survey # 《Diffusion Model-Based Image Editing: A Survey》 论文阅读：Diffusion Model-Based Image Editing: A Survey 基于扩散模型的图像编辑：首篇综述\n《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 300多篇相关研究，复旦、南洋理工最新多模态图像编辑综述论文\n1xx. 《LLMs Meet Multimodal Generation and Editing: A Survey》 * Image Generation， Image Editing Repo\n"},{"id":43,"href":"/www6vVision/docs/Survey/Multimodal/","title":"(综述)多模态","section":"Survey","content":" 论文[Foundational Models Defining] # 论文地址 《Foundational Models Defining a New Era in Vision: A Survey and Outlook》大学 基础模型分类 [1] # 分类 {% asset_img \u0026rsquo;\u0026rsquo; %} 分类 {% asset_img \u0026rsquo;\u0026rsquo; %} textually prompted models # contrastive CLIP 双塔 generative Flamingo hybrid BLIP conversational GPT-4， miniGPT4, LLaVa 传统上，视觉语言模型主要用于需要同时理解视觉和文本模态的任务。然而，随着CLIP展示出的卓越性能，基于语言监督的模型在显著上升，并成为主流方法。在本节中，我们专注于探索依赖语言作为主要监督来源的方法。这些以文本为提示的模型可以广泛分为三种主要类型：对比、生成和混合方法。\nvisually prompted models # Foundational SAM heterogeneous models # 架构 [1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n论文[MM-LLMs] # 论文地址 《MM-LLMs: Recent Advances in MultiModal Large Language Models》 腾讯\n开源地址 mm-llms 腾讯\n解析 解析\n论文[MLLM] # 论文地址 A Survey on Multimodal Large Language Models A Survey on Multimodal Large Language Models 中国科学技术大学 腾讯\n开源地址 Repo\nArch [3.2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n类型[3.1] # 本文将最近具有代表性的MLLM分为4种主要类型： 多模态指令调整（MIT） 多模态上下文学习（M-ICL） 多模态思想链（M-CoT） LLM辅助视觉推理（LAVR）【类似agent】 参考 # survey # 《Foundational Models Defining a New Era in Vision: A Survey and Outlook》 视觉大模型的全面解析 基础模型定义视觉的新时代：综述和展望 万字长文带你全面解读视觉大模型\nxxx\n《A Survey on Multimodal Large Language Models》 v1 v2版本 3.1 MLLM首篇综述 | 一文全览多模态大模型的前世、今生和未来 v1版本 3.2 多模态大语言模型全面综述：架构，训练，数据，评估，扩展，应用，挑战，机遇 v2版本\n"},{"id":44,"href":"/www6vVision/docs/MLLM/VL/Flamingo/","title":"Flamingo","section":"VL","content":" Flamingo[1] # 架构 # 它在Frozen模型的基础上做进一步的改进，不同点主要有两个：一是使用了更大的LLMs，二是冻结视觉编码器，引入perceiver resampler和XAttn-Dense两个适配单元作为可训练的模块。\nperceiver resampler： 类似DETR，通过设计多个Perceiver Resampler来生成64个固定长度的tokens，主要作用在于可以从图像中提取固定长度的特征向量，能够解决图像甚至多帧视频的feature map不一致的问题。【图像和文本对齐】 XAttn-Dense：在每一层LLM上都会增加corss- attention以入到LLM中与视觉向量进行交互，融合多模态信息。【融合】 参考 # Flamingo # 1xx. [论文速览]Flamingo: a Visual Language Model for Few-Shot Learning[2204.14198] V 1xx. DeepMind出手！多模态小样本打败精调 1xx. Otter on OpenFlamingo git 1xx. open_flamingo git\n"},{"id":45,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Instruct/","title":"InstructPix2Pix","section":"Editing","content":" 论文 # 论文地址 InstructPix2Pix: Learning to Follow Image Editing Instructions 开源地址 Repo git Project page Project page Method # 这章就讲两件事：1）如何生成数据集（章节3.1）；2）如何基于上一步生成的训练数据，训练一个图像编辑扩散模型（章节3.2）；\n参考 # InstructPix2Pix：用指令给图像做修改\n实战 # 1xx. InstructPix2Pix diffusers 1xx. InstructPix2Pix V Code Repo git\n"},{"id":46,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/MGIE/","title":"MGIE","section":"Editing","content":" 论文 # 论文地址 MGIE 2024 Apple 开源地址 Repo git\nRepo git Project page Project page Method[1] # 参考 # 【多模态MLLMs+图像编辑】MGIE：苹果开源基于指令和大语言模型的图片编辑神器（24.02.03开源） 1xx. 罕见！苹果开源图片编辑神器MGIE，要上iPhone?\n"},{"id":47,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Pix2pix/","title":"pix2pix-zero","section":"Editing","content":" 论文 # 论文地址\nZero-shot Image-to-Image Translation\n开源地址\npix2pix-zero git\nProject page\nProject page\n官网有个介绍视频 看过不错\nMethod[1] # 上图展示了pix2pix-zero方法的概述，这是一个将图片从猫变成狗的图像到图像的翻译例子。首先，使用规范化的DDIM反转来得到一个反转的噪声映射，这是由BLIP图像字幕（caption）网络和CLIP文本嵌入模型自动生成的文本嵌入引导的。然后，使用原始文本嵌入去噪以获得交叉注意力图，作为输入图像结构的参考（顶部行）。接下来，使用编辑后的文本嵌入去噪，通过损失函数确保这些交叉注意力图与参考交叉注意力图相匹配（第二行）。这确保了编辑图像的结构与原始图像相比不会发生剧烈变化。没有交叉注意力引导的去噪示例显示在第三行，导致结构上的大偏差。此可视化强调了在编辑过程中保持图像原始结构的交叉注意力的重要性。\nDiscovering edit directions [2] # 参考 # pix2pix-zero：零样本图像到图像转换 【深度学习】【风格迁移】Zero-shot Image-to-Image Translation 实战\nZero-shot Image-to-Image Translation\n"},{"id":48,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Prompt/","title":"Prompt-to-Prompt","section":"Editing","content":" 论文 # 论文地址 PROMPT-TO-PROMPT IMAGE EDITING WITH CROSS-ATTENTION CONTROL 开源地址 Prompt-to-Prompt git Project page Project page 示意[1] # 应用[1] # **应用方面：**Prompt-to-Prompt 这个方法是原理上的创新，应用方面只适用于“已经生成了一张大致满意的图，我们想对它进行部分修改”。但是对于“手头有一张来历不明的图，我们想对它进行修改”这个任务就很麻烦了，因为很难去倒推这张图对应的prompt是啥。\n所以后续有一项工作叫 InstructPix2Pix，作用是“一张来历不明的图，只要说‘把猫改成狗’，模型就能把画面里的猫改成狗，其他不变。”非常好用，听说已经集成在 Stable Diffusion WebUI 里可以直接用了。\nMethod[1] # 上半部分，原版cross-attention，下半部分，本文的cross-attention\n参考 # Prompt-to-prompt：让生成的图像保持一致 diffusion model(十四)： prompt-to-prompt 深度剖析\n"},{"id":49,"href":"/www6vVision/docs/%E7%94%9F%E6%88%90/%E4%BA%BA%E5%83%8F%E7%94%9F%E5%9B%BE/IDCreate/","title":"人像生图","section":"人像生图","content":" InstantID # InstantID\nPhotoMaker # PhotoMaker\n总结 # 【InstantID : ipAdaptor +controlnet, image Contoll的思路】\n【photomaker: image Edit 的思路】\n"}]