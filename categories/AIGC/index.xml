<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIGC on Vision</title>
    <link>https://www6v.github.io/www6vVision/categories/AIGC/</link>
    <description>Recent content in AIGC on Vision</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 04 Apr 2024 09:38:25 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vVision/categories/AIGC/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>多模态 系列</title>
      <link>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSeries/</link>
      <pubDate>Thu, 04 Apr 2024 09:38:25 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSeries/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;stage1-模块独立2&#34;&gt;&#xA;  Stage1: 模块独立[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage1-%e6%a8%a1%e5%9d%97%e7%8b%ac%e7%ab%8b2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/stage1.webp&#34; alt=&#34;stage1.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;model&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CLIP&lt;/li&gt;&#xA;&lt;li&gt;ViLT&lt;/li&gt;&#xA;&lt;li&gt;ALBEF&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;stage2-模块共享2&#34;&gt;&#xA;  Stage2: 模块共享[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage2-%e6%a8%a1%e5%9d%97%e5%85%b1%e4%ba%ab2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;model-1&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VLMO&lt;/li&gt;&#xA;&lt;li&gt;BLIP&lt;/li&gt;&#xA;&lt;li&gt;BLIP2&lt;/li&gt;&#xA;&lt;li&gt;BEiTv3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;stage3-范式统一2&#34;&gt;&#xA;  Stage3: 范式统一[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage3-%e8%8c%83%e5%bc%8f%e7%bb%9f%e4%b8%802&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;model-2&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Unified-IO&lt;/li&gt;&#xA;&lt;li&gt;Uni-Perceiver&lt;/li&gt;&#xA;&lt;li&gt;PaLi&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;总结-1&#34;&gt;&#xA;  总结 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/multimodal.webp&#34; alt=&#34;multimodal.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;&#xA;  Overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/653902791&#34;&gt;多模态大模型 CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP 系列解读&lt;/a&gt; ***&lt;/p&gt;</description>
    </item>
    <item>
      <title>VQ-VAE</title>
      <link>https://www6v.github.io/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/VQVAE/</link>
      <pubDate>Sun, 24 Mar 2024 12:06:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/VQVAE/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/647399156&#34;&gt;关于 VQ-VAE 直观理解&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1bb4y1i7j6/?vd_source=f6e8c1128f9f264c5ab8d9411a644036&#34;&gt;[论文简析]VQ-VAE:Neural discrete representation learning[1711.00937]&lt;/a&gt; v&#xA;看评论中的置顶 有代码&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1yN4y1k794/&#34;&gt;如何搭建VQ-VAE模型（Pytorch代码）&lt;/a&gt; v&#xA;看视频介绍 &lt;a href=&#34;https://github.com/KevinOfCathay/DDPM-demo&#34;&gt;https://github.com/KevinOfCathay/DDPM-demo&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>DINO</title>
      <link>https://www6v.github.io/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/DINO/</link>
      <pubDate>Sun, 24 Mar 2024 11:18:19 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3/Vision-Encoder/DINO/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;dino&#34;&gt;&#xA;  DINO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dino&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dino&#34;&gt;https://github.com/facebookresearch/dino&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/EGCAX51FTyZrO7-e4y9Egg&#34;&gt;重塑自监督学习: DINO 网络如何颠覆视觉特征表示的常规方法&lt;/a&gt;   有动图&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;dinov2&#34;&gt;&#xA;  &lt;strong&gt;DINOv2&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dinov2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2304.07193&#34;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;https://github.com/facebookresearch/dinov2&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&#xA;&lt;a href=&#34;https://dinov2.metademolab.com/&#34;&gt;Project page&lt;/a&gt;&#xA;&lt;a href=&#34;https://pierrefdz.github.io/publications/dinov2/&#34;&gt;Project page&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考-1&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2314807&#34;&gt;DINOv2：无需微调，填补 SAM 的空白，支持多个下游任务&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/CVHub/article/details/130304078&#34;&gt;全网最详细的 DINOv2 论文解读来啦！&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43694096/article/details/135761460&#34;&gt;深度学习算法应用实战 | DINOv2 图像相似度实战&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/629042197&#34;&gt;视觉大模型DINOv2:自我监督学习的新领域&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/650234933&#34;&gt;DINOv2：无需微调，填补SAM空白&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mmpretrain.readthedocs.io/zh-cn/dev/papers/dinov2.html&#34;&gt;DINOv2 on mmpretrain&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>CV 任务</title>
      <link>https://www6v.github.io/www6vVision/docs/Vision/VisionTask/</link>
      <pubDate>Tue, 25 Jul 2023 13:49:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Vision/VisionTask/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;分类-1&#34;&gt;&#xA;  分类 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;image-level&#34;&gt;&#xA;  image-level&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#image-level&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;image recognition&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;(Retrieval)image-text retrieval&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Caption(image captioning)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;VQA(visual question answering)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;region-level&#34;&gt;&#xA;  region-level&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#region-level&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Object Detection object detection&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DETR -&amp;gt; DINO -&amp;gt; Grounding DINO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;dense caption&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;phrase grounding&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;pixel-level&#34;&gt;&#xA;  pixel-level&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pixel-level&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Segmentation&#xA;&lt;ul&gt;&#xA;&lt;li&gt;generic segmetation&lt;/li&gt;&#xA;&lt;li&gt;referring segmetation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;其他&#34;&gt;&#xA;  其他&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%85%b6%e4%bb%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;对比&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;[CNN  更深的网络]&lt;/li&gt;&#xA;&lt;li&gt;[transformer 没有局限]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;CV任务&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;分类（Classification）&lt;/li&gt;&#xA;&lt;li&gt;检测（Detection）&lt;/li&gt;&#xA;&lt;li&gt;分割（Segmentation）&lt;/li&gt;&#xA;&lt;li&gt;跟踪（Tracking）&lt;/li&gt;&#xA;&lt;li&gt;行为识别（Action Recognition）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;[&lt;a href=&#34;https://www.bilibili.com/video/BV1ds4y1k7pj/?vd_source=f6e8c1128f9f264c5ab8d9411a644036&#34;&gt;CVPR Tutorial Talk] Towards General Vision Understanding Interface&lt;/a&gt;&#xA;&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Jianwei_CVPR2023_Tutorial.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Agent - UI-assistants</title>
      <link>https://www6v.github.io/www6vVision/docs/Multimodal-Agent/MultimodalAgentApp/</link>
      <pubDate>Fri, 30 Jun 2023 21:12:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Multimodal-Agent/MultimodalAgentApp/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;app-agent&#34;&gt;&#xA;  App Agent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#app-agent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/677071947&#34;&gt;AppAgent源码分析&amp;amp;思考&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/mnotgod96/AppAgent&#34;&gt;https://github.com/mnotgod96/AppAgent&lt;/a&gt;&#xA;&lt;a href=&#34;https://icoz69.github.io/&#34;&gt;https://icoz69.github.io/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/681424409&#34;&gt;【LLM-agent】MOBILE-AGENT: 具有视觉感知能力的自治多模移动设备agent&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/X-PLUG/MobileAgent&#34;&gt;https://github.com/X-PLUG/MobileAgent&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://github.com/OpenAdaptAI/OpenAdapt&#34;&gt;https://github.com/OpenAdaptAI/OpenAdapt&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(survey)多模态  数据集</title>
      <link>https://www6v.github.io/www6vVision/docs/Data/MulitmodalDataset/</link>
      <pubDate>Sat, 01 Apr 2023 15:09:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Data/MulitmodalDataset/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;survey0&#34;&gt;&#xA;  Survey[0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Pre-training&lt;/li&gt;&#xA;&lt;li&gt;Adaptation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;pre-training数据集&#34;&gt;&#xA;  Pre-training数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pre-training%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;LAION[1]&#xA;&lt;a href=&#34;https://laion.ai/projects/&#34;&gt;LAION&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;wukong[1]&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/473794131&#34;&gt;[论文]中文多模态数据集WuKong &amp;amp; FILIP &amp;amp; LiT-tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/551622338&#34;&gt;Wukong：一亿规模的中文跨模态预训练基准&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MMDialog&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/584894471&#34;&gt;百万量级的多模态对话数据集来了，153万张图片4000多主题&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;OBELISC[2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ShareGPT4V[3]&#xA;opensource&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;sft数据集&#34;&gt;&#xA;  SFT数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sft%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LAMM&lt;/li&gt;&#xA;&lt;li&gt;MultiIntruct&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;survey&#34;&gt;&#xA;  survey&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/_fi2odhKITs4fs7MbWpWaw&#34;&gt;多模态模型大常用数据集及处理策略：兼看Chatlaw法律问答中的知识图谱融合思路 &lt;/a&gt;&#xA;《A Survey of Multimodal Large Language Model from A Data-centric Perspective》&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;预训练数据集&#34;&gt;&#xA;  预训练数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/686757824&#34;&gt;多模态数据集收集&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/670149958&#34;&gt;[论文阅读] 开源的多模态文档数据集，OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents&lt;/a&gt;&lt;br&gt;&#xA;从网页文档里得到的数据集&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669485001&#34;&gt;超越同级7B模型！ 中国团队开源大规模高质量图文数据集ShareGPT4V，大幅提升多模态性能&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/527182857&#34;&gt;多模态预训练数据集&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey)多模态</title>
      <link>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSurvey/</link>
      <pubDate>Thu, 16 Mar 2023 12:45:30 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Survey/MultimodalSurvey/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》 .Sep 2023   - microsoft&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings&#34;&gt;Computer Vision in the Wild (CVinW)&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;overview-0&#34;&gt;&#xA;  overview [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/overview.jpeg&#34; alt=&#34;overview.jpeg&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;视觉理解-1&#34;&gt;&#xA;  视觉理解 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%86%e8%a7%89%e7%90%86%e8%a7%a3-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/understanding.png&#34; alt=&#34;understanding.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/understanding-method.png&#34; alt=&#34;understanding-method.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;视觉生成-1&#34;&gt;&#xA;  视觉生成 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%86%e8%a7%89%e7%94%9f%e6%88%90-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;human-alignments-in-visual-generation--10&#34;&gt;&#xA;  Human Alignments in Visual Generation  [10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#human-alignments-in-visual-generation--10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;四种alignment的方式&lt;/p&gt;&#xA;&lt;h3 id=&#34;spatial-controllable-t2i-generation&#34;&gt;&#xA;  spatial controllable T2I generation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#spatial-controllable-t2i-generation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;结合位置分布的文字描述&lt;/strong&gt;（比较麻烦的用户交互，不仅需要文字，而且需要位置），常用于&lt;strong&gt;对位置要求比较高的创意设计（海报等）&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>BLIP-2</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Works/Blip/</link>
      <pubDate>Wed, 15 Mar 2023 23:00:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Works/Blip/</guid>
      <description>&lt;h1 id=&#34;blip-2&#34;&gt;&#xA;  BLIP-2&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#blip-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;overview-1&#34;&gt;&#xA;  Overview [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;用一个Qformer来提取图像特征（等同与Flamingo的perceiver resampler），然后用cross- attention进行多模态交互，此时视觉编码器和LLM都会被冻结，&lt;strong&gt;只训练Qformer&lt;/strong&gt;，而在下游任务微调时，可以再解锁视觉编码器，让它跟Qformer一起训练&lt;/p&gt;&#xA;&lt;h3 id=&#34;两阶段的训练策略-1&#34;&gt;&#xA;  两阶段的训练策略 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%a4%e9%98%b6%e6%ae%b5%e7%9a%84%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;BLIP-2设计了两阶段的训练策略，以使视觉编码器能学会提取更关键的信息。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;第一阶段：使用多种预训练任务，如Image-Text Contrastive Learning(&lt;strong&gt;ITC&lt;/strong&gt;)，Image-grounded Text Generation(&lt;strong&gt;ITG&lt;/strong&gt;)，Image-Text Matching(&lt;strong&gt;ITM&lt;/strong&gt;)让Qformer学会如何从&lt;strong&gt;视觉编码器中抽取文本相关的特征&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;第二阶段，将Qformer插入到LLMs中，用language modeling进行训练。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;架构3&#34;&gt;&#xA;  架构[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%843&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;两个阶段训练&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;阶段一&#xA;获得高质量的 &lt;strong&gt;图文对齐向量表征&lt;/strong&gt;&#xA;通过&lt;strong&gt;ITC ITM  ITG 三个损失函数&lt;/strong&gt;获得了很好的图片文本 &lt;strong&gt;对齐向量表征能力&lt;/strong&gt;，仅训练&lt;strong&gt;Qformer&lt;/strong&gt;中很少的参数&#xA;【ITM:  image-text 是否是匹配的 |    image 和text 都能相互看到】&#xA;【ITG: image生成text |    image 能全看到, text只能逐个的看】&#xA;【ITC: image和text的对比学习, 对比学习分类分错了的  送入ITM 负样本 |  image和 text  之间是不能看到的】&lt;/li&gt;&#xA;&lt;li&gt;阶段二&#xA;通过向量表征进行&lt;strong&gt;文字生成&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;code-2&#34;&gt;&#xA;  code [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#code-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;blip2&#34;&gt;&#xA;  blip2&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#blip2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://nakaizura.blog.csdn.net/article/details/130757157?spm=1001.2014.3001.5502&#34;&gt;基于LLMs的多模态大模型（Flamingo, BLIP-2，KOSMOS-1，ScienceQA）&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(综述)多模态InstructTuning</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Training/InstructTuning/</link>
      <pubDate>Wed, 15 Mar 2023 16:09:00 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Training/InstructTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;datasets-for-visual-instruction-tuning1&#34;&gt;&#xA;  Datasets for Visual Instruction Tuning[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#datasets-for-visual-instruction-tuning1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;single-turn&#34;&gt;&#xA;  Single-turn&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#single-turn&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MiniGPT-4&#xA;&lt;strong&gt;MiniGPT-4&lt;/strong&gt; [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 &lt;strong&gt;randomly selects 5000 images from the Conceptual Caption dataset&lt;/strong&gt; [38], [39] and prompts its &lt;strong&gt;pre-trained VLM model&lt;/strong&gt; to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)MiniGPT4</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Works/Minigpt4/</link>
      <pubDate>Wed, 15 Mar 2023 15:56:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Works/Minigpt4/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;introduction1&#34;&gt;&#xA;  INTRODUCTION[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;MiniGPT-4 增加了一个&lt;strong&gt;投影层&lt;/strong&gt;，将&lt;strong&gt;编码的视觉特征与 Vicuna 语言模型对齐&lt;/strong&gt;，并&lt;strong&gt;冻结了所有其他视觉和语言组件&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  METHOD[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;图 1&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MiniGPT-4 的目标是将来自预训练视觉编码器的视觉信息与先进的大型语言模型（LLM）对齐（Alignment）。具体来说，&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用 &lt;strong&gt;Vicuna作为语言解码器&lt;/strong&gt;，该解码器基于 LLaMA构建，可以执行各种复杂的语言任务。&lt;/li&gt;&#xA;&lt;li&gt;视觉感知方：采用与 &lt;strong&gt;BLIP-2&lt;/strong&gt; 相同的&lt;strong&gt;视觉编码器&lt;/strong&gt;，&lt;strong&gt;ViT Backbone&lt;/strong&gt;及其预先训练好的 &lt;strong&gt;Q-Former&lt;/strong&gt;。&#xA;语言和视觉模型都是开源的。我们的目标是利用线性投影层弥合视觉编码器与 LLM 之间的差距，图 1 显示了模型概览。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;first-pretraining-stage&#34;&gt;&#xA;  FIRST &lt;strong&gt;PRETRAINING&lt;/strong&gt; STAGE&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#first-pretraining-stage&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;第一阶段：在大量对齐的图像-文本对上对模型进行预训练，以获取视觉语言知识。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Traditional alignment method [2]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Input: Image&lt;/li&gt;&#xA;&lt;li&gt;Output: Caption&lt;/li&gt;&#xA;&lt;li&gt;Training Objective: Maximize the likelihood of GT captions&lt;/li&gt;&#xA;&lt;li&gt;Training Dataset 组合数据集 [postprocessed by BLIP]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Conceptual Caption&lt;/li&gt;&#xA;&lt;li&gt;SBU&lt;/li&gt;&#xA;&lt;li&gt;LAION&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;curating-a-high-quality-alignment-dataset-for-vision-language-domain&#34;&gt;&#xA;  CURATING A &lt;strong&gt;HIGH-QUALITY ALIGNMENT DATASET&lt;/strong&gt; FOR VISION-LANGUAGE DOMAIN.&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#curating-a-high-quality-alignment-dataset-for-vision-language-domain&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create a dataset with detailed, human-perfered descriptions[2][1]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;model  generates descriptions&#xA;在初始阶段，我们使用从第一个预训练阶段得到的模型来&lt;strong&gt;生成输入图像的描述&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;li&gt;polishing and filtering by chatgpt&#xA;上述自动生成的图片说明包含&lt;strong&gt;噪音或不连贯的描述&lt;/strong&gt;，例如单词或句子重复，句子支离破碎或内容不相关。为了解决这些问题，我们采用了&lt;strong&gt;ChatGPT&lt;/strong&gt;，通过以下提示对描述进行&lt;strong&gt;修补&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;further polishing and filtering by rules &amp;amp; human&#xA;完成后处理阶段后，我们会手动验证每张图片说明的正确性，以保证其高质量。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;second-stage-finetuning&#34;&gt;&#xA;  SECOND-STAGE &lt;strong&gt;FINETUNING&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#second-stage-finetuning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;第二阶段：使用一个较小但高质量的图像-文本数据集对预训练模型进行微调，并设计了对话模板，以提高生成的可靠性和可用性。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;【blip2能识别图像，但是对话能力比较弱，不能说出图像中的细节。在pre-train阶段获取视觉语言知识， 在fine-tuning 阶段获取对话能力】  [2]&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战) LLaVa 演化</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Works/Llava/</link>
      <pubDate>Tue, 14 Mar 2023 23:02:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Works/Llava/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;llava-演化&#34;&gt;&#xA;  LLaVa 演化&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llava-%e6%bc%94%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaVa-cef875377c394636a64cf57edbb0026e?pvs=4&#34;&gt;(原理|实战) LLaVa 演化&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;llava-实战&#34;&gt;&#xA;  LLaVa 实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llava-%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaVa-0bf9f127dc7c41e796050bcb8f7fb1b3?pvs=4&#34;&gt;(实战) LLaVa &lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>(原理)Web Agent</title>
      <link>https://www6v.github.io/www6vVision/docs/Multimodal-Agent/WebAgent/</link>
      <pubDate>Sun, 05 Mar 2023 10:30:03 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Multimodal-Agent/WebAgent/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;web-scenarios-1&#34;&gt;&#xA;  web scenarios [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#web-scenarios-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;在网络场景中，代表用户执行特定任务被称为Web导航问题[390]。代理程序解释用户指令，将其分解为多个基本操作，并与计算机进行交互。这通常涉及到填写表单、在线购物和发送电子邮件等网络任务。代理程序需要具备理解复杂网络场景中的指令的能力，适应变化（如嘈杂的文本和动态HTML网页），并推广成功的操作[391]。通过这种方式，代理程序可以在处理未知任务时实现可访问性和自动化[435]，最终使人类免于与计算机用户界面的重复交互。&lt;/p&gt;&#xA;&lt;p&gt;通过强化学习训练的代理程序可以有效地模仿人类行为，使用预定义的操作，如键入、搜索、导航到下一页等。它们在基本任务（如在线购物[392]和搜索引擎检索[90]）中表现良好，这些任务已经得到广泛探索。然而，没有语言模型能力的代理程序可能难以适应现实世界互联网中更真实和复杂的场景。在动态、内容丰富的网页上，如在线论坛或在线业务管理[391]，代理程序常常面临性能方面的挑战。&lt;/p&gt;&#xA;&lt;p&gt;为了实现代理程序与更真实的网页之间的成功交互，一些研究人员[393；394]开始利用语言模型的强大HTML读取和理解能力。通过设计提示，他们试图使代理程序理解整个HTML源代码，并预测更合理的下一步操作。Mind2Web[389]结合了为HTML进行微调的多个语言模型，使它们能够在现实世界的场景中总结冗长的HTML代码[388]并提取有价值的信息。此外，WebGum[390]通过使用包含HTML截屏的多模态语料库，赋予代理程序视觉感知能力。它同时进行了语言模型和视觉编码器的微调，加深了代理程序对网页的全面理解。&lt;/p&gt;&#xA;&lt;p&gt;Performing specific tasks on behalf of users in a web scenario is known as the web navigation problem [390]. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations [391]. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future [435], ultimately freeing humans from repeated interactions with computer UIs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)多模态预训练 概述</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Training/Pretrain/</link>
      <pubDate>Sat, 04 Mar 2023 13:23:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/Training/Pretrain/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;overview&#34;&gt;&#xA;  Overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/overview.png&#34; alt=&#34;overview.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;多模态预训练&#34;&gt;&#xA;  多模态预训练&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81%e9%a2%84%e8%ae%ad%e7%bb%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;数据集&#34;&gt;&#xA;  数据集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;大规模无标注&lt;/li&gt;&#xA;&lt;li&gt;内容杂  噪音多&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;架构transformer&#34;&gt;&#xA;  架构Transformer&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84transformer&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于transformer encoder-理解任务&#xA;单流 - vl-bert  UNITER&#xA;双流 - ViLBERT， CLIP（双流结构，对比学习）&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于transformer decoder-生成任务&#xA;DALL-E  （VQVAE+GPT,  Text-to-Image Generation）&#xA;现在都用 → SD 扩散模型&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;基于encoder+decoder-理解+生成&#xA;文本的decoder&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;encoder + decoder 串行,  交叉注意力&lt;/li&gt;&#xA;&lt;li&gt;encoder + decoder 并行&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;模型---自监督学习&#34;&gt;&#xA;  模型 - 自监督学习&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a8%a1%e5%9e%8b---%e8%87%aa%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;模态内掩码学习&#xA;文本 语音 视觉自身token级别mask&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Agent 多模态</title>
      <link>https://www6v.github.io/www6vVision/docs/Multimodal-Agent/MultimodalAgent/</link>
      <pubDate>Tue, 21 Feb 2023 10:10:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/Multimodal-Agent/MultimodalAgent/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/abs/2402.15116&#34;&gt;《Large Multimodal Agents: A Survey》&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/jun0wanan/awesome-large-multimodal-agents&#34;&gt;Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;survey&#34;&gt;&#xA;  Survey&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;类型-i无长期记忆的闭源-llms-作为规划器&#34;&gt;&#xA;  类型 I：无长期记忆的闭源 LLMs 作为规划器。&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b-i%e6%97%a0%e9%95%bf%e6%9c%9f%e8%ae%b0%e5%bf%86%e7%9a%84%e9%97%ad%e6%ba%90-llms-%e4%bd%9c%e4%b8%ba%e8%a7%84%e5%88%92%e5%99%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.04671.pdf&#34;&gt;&lt;strong&gt;Visual ChatGPT&lt;/strong&gt;&lt;/a&gt;  ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.11381.pdf&#34;&gt;&lt;strong&gt;MM-REACT&lt;/strong&gt;&lt;/a&gt;  ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.08128.pdf&#34;&gt;&lt;strong&gt;ViperGPT&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.17580.pdf&#34;&gt;&lt;strong&gt;HuggingGPT&lt;/strong&gt;&lt;/a&gt;  ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.09842.pdf&#34;&gt;&lt;strong&gt;Chameleon&lt;/strong&gt;&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.00571.pdf&#34;&gt;&lt;strong&gt;LLaVA-Interactive&lt;/strong&gt;&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.01614&#34;&gt;&lt;strong&gt;SeeAct&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.01415.pdf&#34;&gt;&lt;strong&gt;GPT-Driver&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.16158.pdf&#34;&gt;&lt;strong&gt;Mobile-Agent&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;类型-ii无长期记忆的微调-llms-作为规划器&#34;&gt;&#xA;  类型 II：无长期记忆的微调 LLMs 作为规划器。&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b-ii%e6%97%a0%e9%95%bf%e6%9c%9f%e8%ae%b0%e5%bf%86%e7%9a%84%e5%be%ae%e8%b0%83-llms-%e4%bd%9c%e4%b8%ba%e8%a7%84%e5%88%92%e5%99%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.08640.pdf&#34;&gt;&lt;strong&gt;LLaVA-Plus&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.18752.pdf&#34;&gt;&lt;strong&gt;GPT4Tools&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;类型-iv具有本地长期记忆的规划器&#34;&gt;&#xA;  类型 IV：具有本地长期记忆的规划器。&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b-iv%e5%85%b7%e6%9c%89%e6%9c%ac%e5%9c%b0%e9%95%bf%e6%9c%9f%e8%ae%b0%e5%bf%86%e7%9a%84%e8%a7%84%e5%88%92%e5%99%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.05997.pdf&#34;&gt;&lt;strong&gt;JARV IS-1&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.13771.pdf&#34;&gt;&lt;strong&gt;AppAgent&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.07162.pdf&#34;&gt;&lt;strong&gt;DLAH&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;多模态-agent1&#34;&gt;&#xA;  多模态 Agent[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81-agent1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;核心组件&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;感知&lt;/strong&gt;组件关注处理多模态信息&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;规划器&lt;/strong&gt;负责推理和制定计划&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;行动&lt;/strong&gt;组件执行计划&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;记忆&lt;/strong&gt;组件则涉及长期和短期记忆&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;四种类型&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;无长期记忆的闭源 LLMs 作为规划器&lt;/li&gt;&#xA;&lt;li&gt;无长期记忆的微调 LLMs 作为规划器&lt;/li&gt;&#xA;&lt;li&gt;具有间接长期记忆的规划器&lt;/li&gt;&#xA;&lt;li&gt;具有本地长期记忆的规划器&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;多智能体协作&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
