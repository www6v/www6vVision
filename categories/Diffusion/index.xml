<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Diffusion on Vision</title>
    <link>https://www6v.github.io/www6vVision/categories/Diffusion/</link>
    <description>Recent content in Diffusion on Vision</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 17 Jul 2024 10:51:11 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vVision/categories/Diffusion/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(原理|实战)DreamBooth</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/DreamBooth/</link>
      <pubDate>Wed, 17 Jul 2024 10:51:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/DreamBooth/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;approach1&#34;&gt;&#xA;  Approach[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#approach1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Our method takes as input a few images (typically 3-5 images suffice, based on our experiments) of a subject (e.g., a specific dog) and the corresponding class name (e.g. &amp;ldquo;dog&amp;rdquo;), and returns a fine-tuned/&amp;ldquo;personalized&amp;rsquo;&amp;rsquo; text-to-image model that encodes a &lt;strong&gt;unique identifier that refers to the subject&lt;/strong&gt;. Then, at inference, we can implant the unique identifier in different sentences to synthesize the subjects in difference contexts.&#xA;我们的方法将主题（例如，特定的狗）和相应的类名称（例如“狗”）的一些图像（根据我们的实验，通常 3-5 个图像就足够了）作为输入，并返回一个微调/ “个性化”文本到图像模型，编码&lt;strong&gt;指向主题的唯一标识符&lt;/strong&gt;。然后，在推理时，我们可以将唯一标识符植入不同的句子中，以合成不同上下文中的主题。&lt;/p&gt;</description>
    </item>
    <item>
      <title>pix2pix-zero</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Pix2pix/</link>
      <pubDate>Tue, 16 Apr 2024 16:26:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Pix2pix/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2302.03027&#34;&gt;Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/pix2pixzero/pix2pix-zero&#34;&gt;pix2pix-zero&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&lt;br&gt;&#xA;&lt;a href=&#34;https://pix2pixzero.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;官网有个介绍视频 看过不错&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  Method[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/e9wsarfq.bmp&#34; alt=&#34;e9wsarfq.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;上图展示了pix2pix-zero方法的概述，这是一个将图片从猫变成狗的图像到图像的翻译例子。首先，使用规范化的DDIM反转来得到一个反转的噪声映射，这是由BLIP图像字幕（caption）网络和CLIP文本嵌入模型自动生成的文本嵌入引导的。然后，使用原始文本嵌入去噪以获得交叉注意力图，作为输入图像结构的参考（顶部行）。接下来，使用编辑后的文本嵌入去噪，通过损失函数确保这些交叉注意力图与参考交叉注意力图相匹配（第二行）。这确保了编辑图像的结构与原始图像相比不会发生剧烈变化。没有交叉注意力引导的去噪示例显示在第三行，导致结构上的大偏差。此可视化强调了在编辑过程中保持图像原始结构的交叉注意力的重要性。&lt;/p&gt;&#xA;&lt;h1 id=&#34;discovering-edit-directions-2&#34;&gt;&#xA;  Discovering edit directions [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#discovering-edit-directions-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/hwe1rl5e.bmp&#34; alt=&#34;hwe1rl5e.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/684673737&#34;&gt;pix2pix-zero：零样本图像到图像转换&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/x1131230123/article/details/132169755&#34;&gt;【深度学习】【风格迁移】Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;实战&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/docs/diffusers/v0.13.0/en/api/pipelines/stable_diffusion/pix2pix_zero&#34;&gt;Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>InstructPix2Pix</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Instruct/</link>
      <pubDate>Tue, 16 Apr 2024 16:25:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Instruct/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2211.09800&#34;&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;Repo&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;method&#34;&gt;&#xA;  &lt;strong&gt;Method&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;这章就讲两件事：1）如何&lt;strong&gt;生成数据集&lt;/strong&gt;（章节3.1）；2）如何&lt;strong&gt;基于上一步生成的训练数据，训练一个图像编辑扩散模型&lt;/strong&gt;（章节3.2）；&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/94xbrqlt.bmp&#34; alt=&#34;94xbrqlt.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/72gzhwfl.bmp&#34; alt=&#34;72gzhwfl.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/655135961&#34;&gt;InstructPix2Pix：用指令给图像做修改&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.notion.so/5d04c1bbd0d34be4b3fb0087cb670efd?pvs=21&#34;&gt;InstructPix2Pix&lt;/a&gt;  diffusers&#xA;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1Go4y1M7cK?p=3&#34;&gt;InstructPix2Pix&lt;/a&gt; V&#xA;&lt;a href=&#34;https://github.com/www6v/Diffusion_Training_Examples&#34;&gt;Code Repo&lt;/a&gt; git&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prompt-to-Prompt</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Prompt/</link>
      <pubDate>Tue, 16 Apr 2024 16:25:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/Prompt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;&lt;a href=&#34;https://prompt-to-prompt.github.io/ptp_files/Prompt-to-Prompt_preprint.pdf&#34;&gt;PROMPT-TO-PROMPT IMAGE EDITING&#xA;WITH CROSS-ATTENTION CONTROL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/google/prompt-to-prompt/&#34;&gt;Prompt-to-Prompt&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://prompt-to-prompt.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;示意1&#34;&gt;&#xA;  示意[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%a4%ba%e6%84%8f1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/gvn5hr6b.bmp&#34; alt=&#34;gvn5hr6b.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;应用1&#34;&gt;&#xA;  应用[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;**应用方面：**Prompt-to-Prompt 这个方法是原理上的创新，应用方面只适用于“已经生成了一张大致满意的图，我们想对它进行部分修改”。&lt;strong&gt;但是对于“手头有一张来历不明的图，我们想对它进行修改”这个任务就很麻烦了&lt;/strong&gt;，因为很难去倒推这张图对应的prompt是啥。&lt;/p&gt;&#xA;&lt;p&gt;所以后续有一项工作叫 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/655135961&#34;&gt;InstructPix2Pix&lt;/a&gt;，作用是“&lt;strong&gt;一张来历不明的图，只要说‘把猫改成狗’，模型就能把画面里的猫改成狗，其他不变&lt;/strong&gt;。”非常好用，听说已经集成在 Stable Diffusion WebUI 里可以直接用了。&lt;/p&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  Method[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/vfk2k2bg.bmp&#34; alt=&#34;vfk2k2bg.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;上半部分，原版cross-attention，下半部分，本文的cross-attention&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/ck0g81ot.bmp&#34; alt=&#34;ck0g81ot.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/655372592&#34;&gt;Prompt-to-prompt：让生成的图像保持一致&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_40779727/article/details/136854062&#34;&gt;diffusion model(十四)： prompt-to-prompt 深度剖析&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)ReferenceNet</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/ReferenceNet/</link>
      <pubDate>Tue, 22 Aug 2023 18:27:16 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/ReferenceNet/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;referencenet&#34;&gt;&#xA;  ReferenceNet&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#referencenet&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/ReferenceNet-22a14d3a669e4d7f8c99409e34252349?pvs=4&#34;&gt;(原理|实战)ReferenceNet&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)IP-Adapter</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/IPAdapter/</link>
      <pubDate>Tue, 22 Aug 2023 18:26:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/IPAdapter/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文ip-adapter&#34;&gt;&#xA;  论文[IP-Adapter]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87ip-adapter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2308.06721&#34;&gt;IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt; git&#xA;enable a pretrained text-to-image diffusion model to generate images &lt;strong&gt;with image prompt&lt;/strong&gt;&#xA;有很多notebook的demo&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://ip-adapter.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;ip-adapter10&#34;&gt;&#xA;  IP-Adapter[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ip-adapter10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hf.co/papers/2308.06721&#34;&gt;IP-Adapter&lt;/a&gt; is an image prompt adapter that can be plugged into diffusion models to&#xA;enable image prompting without any changes to the underlying model.&#xA;Furthermore, this adapter can be reused with other models finetuned from&#xA;the same base model and it can be combined with other adapters like &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlnet&#34;&gt;ControlNet&lt;/a&gt;. The key idea behind IP-Adapter is the &lt;em&gt;decoupled cross-attention&lt;/em&gt;&#xA;mechanism which adds a separate cross-attention layer just for image&#xA;features instead of using the same cross-attention layer for both text&#xA;and image features. This allows the model to learn more image-specific&#xA;features.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)T2I-Adapter</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/T2IAdapter/</link>
      <pubDate>Tue, 22 Aug 2023 18:26:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/T2IAdapter/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文t2i-adapter&#34;&gt;&#xA;  论文[T2I-Adapter]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87t2i-adapter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2302.08453&#34;&gt;T2I-Adapter&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/TencentARC/T2I-Adapter&#34;&gt;T2I-Adapter&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;t2i-adapter&#34;&gt;&#xA;  &lt;strong&gt;T2I-Adapter[10]&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#t2i-adapter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hf.co/papers/2302.08453&#34;&gt;T2I-Adapter&lt;/a&gt; is a lightweight adapter for controlling and providing more accurate&#xA;structure guidance for text-to-image models. It works by learning an alignment between the internal knowledge of the&#xA;text-to-image model and an external control signal, such as edge detection or depth estimation.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hf.co/papers/2302.08453&#34;&gt;T2I-Adapter&lt;/a&gt; 是一种&lt;strong&gt;轻量级&lt;/strong&gt;适配器，用于控制&lt;strong&gt;文本到图像&lt;/strong&gt;模型并提供更准确的&lt;strong&gt;结构指导&lt;/strong&gt;。它的工作原理是学习文本到图像模型的&lt;strong&gt;内部知识与外部控制信号&lt;/strong&gt;（如边缘检测或深度估计）之间的&lt;strong&gt;对齐&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;The T2I-Adapter design is simple, the condition is passed to four feature extraction blocks and three downsample blocks. This makes it fast and easy to train different adapters for different conditions which can be plugged into the text-to-image model. T2I-Adapter is similar to &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlnet&#34;&gt;ControlNet&lt;/a&gt; except it is smaller (~77M parameters) and faster because it only runs once during the diffusion process. The downside is that performance may be slightly worse than ControlNet.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战)ControlNet</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/ControlNet/</link>
      <pubDate>Tue, 22 Aug 2023 18:26:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/ControlNet/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文controlnet&#34;&gt;&#xA;  论文[ControlNet]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87controlnet&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/abs/2302.05543&#34;&gt;Adding Conditional Control to Text-to-Image Diffusion Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;controlnet10&#34;&gt;&#xA;  ControlNet[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#controlnet10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;ControlNet is a type of model for controlling image diffusion models by&#xA;conditioning the model with an additional input image. There are many&#xA;types of conditioning inputs (canny edge, user sketching, human pose,&#xA;depth, and more) you can use to control a diffusion model. This is&#xA;hugely useful because it affords you greater control over image&#xA;generation, making it easier to generate specific images without&#xA;experimenting with different text prompts or denoising values as much.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work|实战)Image Editing</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEditWork/</link>
      <pubDate>Thu, 27 Jul 2023 09:29:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEditWork/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;总结&#34;&gt;&#xA;  总结&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prompt-to-Prompt&lt;br&gt;&#xA;train-free，察觉到了attention map的妙用&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;pix2pix-zero&lt;br&gt;&#xA;察觉到了attention map的妙用&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;InstructPix2Pix&lt;br&gt;&#xA;trainable，training数据基于Prompt-to-Prompt&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MGIE&lt;br&gt;&#xA;基于LMM&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>(work|实战) fine-tuning</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/FineTuning/</link>
      <pubDate>Thu, 06 Jul 2023 19:24:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Controllable/FineTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;对比总结1&#34;&gt;&#xA;  对比总结[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%931&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;训练方法&lt;/th&gt;&#xA;          &lt;th&gt;方法&lt;/th&gt;&#xA;          &lt;th&gt;局限性&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Text Inversion&lt;/td&gt;&#xA;          &lt;td&gt;使用提供的一组图片&lt;strong&gt;训练一个新单词的Embedding&lt;/strong&gt; ，并将其与词汇表中的已有单词关联起来，这个新单词即为这组图片概念的指代。&lt;/td&gt;&#xA;          &lt;td&gt;训练过程只对应 Embedding，扩散模型没有新知识输入，所以也无法产生新的内容。&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Full FineTune&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;最朴素&lt;/strong&gt;的方式，使用图片+ 标注的数据集，进行迭代训练，数据集标注可以选择BLIP来生成。训练直接对原模型的所有权重进行调整。&lt;/td&gt;&#xA;          &lt;td&gt;容易过拟合，导致生成图片的多样性不够，结果难以控制。模型体积大，不便于传播。&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Dreambooth&lt;/td&gt;&#xA;          &lt;td&gt;提供代表某个新概念（instance） 对应的一组图像，并使用&lt;strong&gt;罕见字符（identifier）&lt;/strong&gt; 进行概念Mapping，训练过程充分考虑&lt;strong&gt;原有相关主题（class）生成&lt;/strong&gt;，避免过拟合。训练直接对&lt;strong&gt;原模型的所有权重进行调整&lt;/strong&gt;。&lt;/td&gt;&#xA;          &lt;td&gt;训练过程只针对新概念 （instance），&lt;strong&gt;多样性差&lt;/strong&gt;。如果需要多概念生成，需要多次训练。模型体积大，不便于传播。&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;LoRA（w Dreambooth）&lt;/td&gt;&#xA;          &lt;td&gt;冻结预训练模型参数，在每个Transformer块插入&lt;strong&gt;可训练层&lt;/strong&gt;，不需要完整调整 UNet 模型的全部参数。&lt;strong&gt;训练结果只保留新增的网络层，模型体积小&lt;/strong&gt;。&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;训练效果不如Dreambooth&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h1 id=&#34;对比总结2&#34;&gt;&#xA;  对比总结[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%932&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/fine-tuning.jpg&#34; alt=&#34;fine-tuning.jpg&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;对比总结3&#34;&gt;&#xA;  对比总结[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%933&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/gmcjfrdl.bmp&#34; alt=&#34;gmcjfrdl.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2302436&#34;&gt;Stable Diffusion 微调及推理优化&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV19h4y1475y/&#34;&gt;【论文串读】Stable Diffusion模型微调方法串读&lt;/a&gt; V&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669895990&#34;&gt;Stable Diffusion——四种模型 LoRA（包括LyCORIS）、Embeddings、Dreambooth、Hypernetwork&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1184y1g7pG/?p=4&#34;&gt;Text Inversion&lt;/a&gt; V&#xA;【这个比较详细】&#xA;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1184y1g7pG?p=7&#34;&gt;lora Dreambooth&lt;/a&gt; V&#xA;【冻结不训练unet，只训练lora】&#xA;【为unet模型添加注意力层，注意力层是要训练的参数】&#xA;【大部分代码和Dreambooth差不多】&lt;/p&gt;&#xA;&lt;h3 id=&#34;实战-1&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx.  &lt;a href=&#34;https://www.notion.so/concept-customization-067033e842b044729d81aed1d96608fd?pvs=21&#34;&gt;+ concept customization&lt;/a&gt;  &lt;br&gt;&#xA;dreambooth lora + textual_inversion   diffusers&lt;/p&gt;</description>
    </item>
    <item>
      <title>(综述)Image Editing</title>
      <link>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEdit/</link>
      <pubDate>Thu, 06 Jul 2023 19:10:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vVision/docs/%E7%94%9F%E6%88%90/Editing/ImageEdit/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;&#xA;  目录&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;&lt;a href=&#34;https://arxiv.org/abs/2402.17525&#34;&gt;《Diffusion Model-Based Image Editing: A Survey》&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods&#34;&gt;Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;图像编辑1&#34;&gt;&#xA;  图像编辑[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9b%be%e5%83%8f%e7%bc%96%e8%be%911&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;大类&#34;&gt;&#xA;  大类&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e7%b1%bb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从图片编辑的任务方面可以被分为3个大类&#xA;&lt;ul&gt;&#xA;&lt;li&gt;语义编辑semantic editing&lt;/li&gt;&#xA;&lt;li&gt;风格编辑stylistic editing&lt;/li&gt;&#xA;&lt;li&gt;结构编辑structural editing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;approaches&#34;&gt;&#xA;  APPROACHES&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#approaches&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TRAINING-BASED APPROACHES&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;InstructPix2Pix&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TESTING-TIME FINETUNING APPROACHES&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TRAINING AND FINETUNING FREE APPROACHES&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;论文2&#34;&gt;&#xA;  论文[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%872&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;论文地址&#xA;《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 复旦、南洋理工&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;开源地址&#xA;&lt;a href=&#34;https://github.com/xinchengshuai/Awesome-Image-Editing&#34;&gt;A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
